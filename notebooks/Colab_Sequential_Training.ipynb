{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be586a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check GPU\n",
    "import torch\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available(): print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "else: raise RuntimeError('Enable GPU in Runtime > Change runtime type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8733a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install packages\n",
    "!pip install -q nltk gensim pillow tqdm\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "print('Packages installed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa989f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f690f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Copy data (only processed data needed, not raw WikiArt)\n",
    "import os\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "!mkdir -p /content/artemis\n",
    "!cp -r \"{DRIVE_PATH}/data\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/utils\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/models\" /content/artemis/\n",
    "!cp \"{DRIVE_PATH}/train.py\" /content/artemis/\n",
    "print('Data copied!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Setup paths\n",
    "import sys\n",
    "os.chdir('/content/artemis')\n",
    "sys.path.insert(0, '/content/artemis')\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34016e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Imports\n",
    "import torch, json, time, gc\n",
    "import numpy as np\n",
    "from utils.text_preprocessing import TextPreprocessor\n",
    "from utils.data_loader import create_data_loaders\n",
    "from models.cnn_lstm import create_model as create_cnn_model\n",
    "from models.vision_transformer import VisionTransformerCaptioning\n",
    "from train import Trainer\n",
    "print('All imports OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define all 4 training configurations\n",
    "CONFIGS = {\n",
    "    1: {\n",
    "        'name': 'colab_cnn_high_lr',\n",
    "        'model_type': 'cnn_lstm',\n",
    "        'description': 'CNN+LSTM High LR - Best from training analysis',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 30,\n",
    "        'learning_rate': 3e-4,\n",
    "        'embed_dim': 512,\n",
    "        'hidden_dim': 1024,\n",
    "        'attention_dim': 512,\n",
    "        'dropout': 0.4,\n",
    "    },\n",
    "    2: {\n",
    "        'name': 'colab_cnn_glove',\n",
    "        'model_type': 'cnn_lstm',\n",
    "        'description': 'CNN+LSTM with GloVe Embeddings',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 30,\n",
    "        'learning_rate': 2e-4,\n",
    "        'embed_dim': 300,  # GloVe dimension\n",
    "        'hidden_dim': 512,\n",
    "        'attention_dim': 256,\n",
    "        'dropout': 0.3,\n",
    "        'use_glove': True,\n",
    "    },\n",
    "    3: {\n",
    "        'name': 'colab_vit_standard',\n",
    "        'model_type': 'vit',\n",
    "        'description': 'Vision Transformer Standard (6 layers)',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 25,\n",
    "        'learning_rate': 2e-4,\n",
    "        'embed_dim': 256,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 6,\n",
    "        'ff_dim': 1024,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    4: {\n",
    "        'name': 'colab_vit_compact',\n",
    "        'model_type': 'vit',\n",
    "        'description': 'Vision Transformer Compact (4 layers, higher LR)',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 25,\n",
    "        'learning_rate': 3e-4,\n",
    "        'embed_dim': 256,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 4,\n",
    "        'ff_dim': 512,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "}\n",
    "\n",
    "print('Configurations defined:')\n",
    "for k, v in CONFIGS.items():\n",
    "    print(f\"  {k}: {v['name']} - {v['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Helper class for limiting batches\n",
    "class LimitedLoader:\n",
    "    def __init__(self, loader, max_batches):\n",
    "        self.loader = loader\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_size = loader.batch_size\n",
    "        self.dataset = loader.dataset\n",
    "    def __iter__(self):\n",
    "        for i, b in enumerate(self.loader):\n",
    "            if i >= self.max_batches: break\n",
    "            yield b\n",
    "    def __len__(self): return min(len(self.loader), self.max_batches)\n",
    "\n",
    "print('LimitedLoader defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cc5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Load vocabulary (shared across all models)\n",
    "text_proc = TextPreprocessor()\n",
    "text_proc.load_vocabulary('data/processed/vocabulary.json')\n",
    "print(f'Vocabulary size: {text_proc.vocab_size}')\n",
    "\n",
    "# Load GloVe embeddings for config 2\n",
    "glove_matrix = None\n",
    "if os.path.exists('data/embeddings/glove_embeddings.npy'):\n",
    "    glove_matrix = np.load('data/embeddings/glove_embeddings.npy')\n",
    "    print(f'GloVe embeddings loaded: {glove_matrix.shape}')\n",
    "else:\n",
    "    print('Warning: GloVe embeddings not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Training function\n",
    "def train_config(config_num, text_proc, glove_matrix=None):\n",
    "    \"\"\"Train a single configuration.\"\"\"\n",
    "    config = CONFIGS[config_num]\n",
    "    name = config['name']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"STARTING CONFIG {config_num}: {name}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    loaders = create_data_loaders(\n",
    "        text_preprocessor=text_proc,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=2,\n",
    "        splits=['train', 'val']\n",
    "    )\n",
    "    train_loader = LimitedLoader(loaders['train'], config['num_images'] // config['batch_size'])\n",
    "    val_loader = LimitedLoader(loaders['val'], 50)\n",
    "    print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n",
    "    \n",
    "    # Create model\n",
    "    if config['model_type'] == 'cnn_lstm':\n",
    "        if config.get('use_glove') and glove_matrix is not None:\n",
    "            model = create_cnn_model(\n",
    "                embedding_matrix=glove_matrix,\n",
    "                vocab_size=text_proc.vocab_size,\n",
    "                embed_dim=config['embed_dim'],\n",
    "                decoder_dim=config['hidden_dim'],\n",
    "                attention_dim=config['attention_dim'],\n",
    "                dropout=config['dropout']\n",
    "            )\n",
    "        else:\n",
    "            model = create_cnn_model(\n",
    "                vocab_size=text_proc.vocab_size,\n",
    "                embed_dim=config['embed_dim'],\n",
    "                decoder_dim=config['hidden_dim'],\n",
    "                attention_dim=config['attention_dim'],\n",
    "                dropout=config['dropout']\n",
    "            )\n",
    "    else:  # vit\n",
    "        model = VisionTransformerCaptioning(\n",
    "            vocab_size=text_proc.vocab_size,\n",
    "            embed_dim=config['embed_dim'],\n",
    "            num_heads=config['num_heads'],\n",
    "            num_encoder_layers=config['num_layers'],\n",
    "            num_decoder_layers=config['num_layers'],\n",
    "            ff_dim=config['ff_dim'],\n",
    "            max_seq_len=30,\n",
    "            dropout=config['dropout'],\n",
    "            img_size=128,\n",
    "            patch_size=16\n",
    "        )\n",
    "    \n",
    "    model = model.to('cuda')\n",
    "    print(f'Model params: {sum(p.numel() for p in model.parameters()):,}')\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(f\"checkpoints/{name}\", exist_ok=True)\n",
    "    os.makedirs(f\"outputs/{name}\", exist_ok=True)\n",
    "    \n",
    "    # Train\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        text_preprocessor=text_proc,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        device='cuda',\n",
    "        checkpoint_dir=f\"checkpoints/{name}\",\n",
    "        output_dir=f\"outputs/{name}\"\n",
    "    )\n",
    "    \n",
    "    t0 = time.time()\n",
    "    history = trainer.train(num_epochs=config['epochs'])\n",
    "    duration = (time.time() - t0) / 60\n",
    "    \n",
    "    # Results\n",
    "    best_bleu = max(history['val_bleu'])\n",
    "    best_loss = min(history['val_loss'])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPLETED: {name}\")\n",
    "    print(f\"Duration: {duration:.1f} min\")\n",
    "    print(f\"Best BLEU: {best_bleu:.4f}\")\n",
    "    print(f\"Best Loss: {best_loss:.4f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Save to Drive immediately\n",
    "    DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "    os.system(f'mkdir -p \"{DRIVE_PATH}/checkpoints\"')\n",
    "    os.system(f'mkdir -p \"{DRIVE_PATH}/outputs\"')\n",
    "    os.system(f'cp -r \"checkpoints/{name}\" \"{DRIVE_PATH}/checkpoints/\"')\n",
    "    os.system(f'cp -r \"outputs/{name}\" \"{DRIVE_PATH}/outputs/\"')\n",
    "    print(f'Saved to Drive: {DRIVE_PATH}')\n",
    "    \n",
    "    # Cleanup GPU memory\n",
    "    del model, trainer, train_loader, val_loader, loaders\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {'name': name, 'duration': duration, 'best_bleu': best_bleu, 'best_loss': best_loss}\n",
    "\n",
    "print('Training function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. RUN ALL 4 CONFIGURATIONS SEQUENTIALLY\n",
    "all_results = []\n",
    "total_start = time.time()\n",
    "\n",
    "for config_num in [1, 2, 3, 4]:\n",
    "    try:\n",
    "        result = train_config(config_num, text_proc, glove_matrix)\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in config {config_num}: {e}\")\n",
    "        all_results.append({'name': CONFIGS[config_num]['name'], 'error': str(e)})\n",
    "\n",
    "total_duration = (time.time() - total_start) / 60\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"ALL TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total time: {total_duration:.1f} minutes ({total_duration/60:.1f} hours)\")\n",
    "print(\"\\nResults Summary:\")\n",
    "print(\"-\" * 50)\n",
    "for r in all_results:\n",
    "    if 'error' in r:\n",
    "        print(f\"  {r['name']}: ERROR - {r['error']}\")\n",
    "    else:\n",
    "        print(f\"  {r['name']}: BLEU={r['best_bleu']:.4f}, Loss={r['best_loss']:.4f}, Time={r['duration']:.1f}min\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Save final summary to Drive\n",
    "import json\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "\n",
    "summary = {\n",
    "    'total_duration_minutes': total_duration,\n",
    "    'results': all_results,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open(f'{DRIVE_PATH}/colab_training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary saved to {DRIVE_PATH}/colab_training_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
