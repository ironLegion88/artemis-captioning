{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3be586a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3be586a5",
        "outputId": "7f8bba6e-51ce-49ff-e619-25a3946893df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# 1. Check GPU\n",
        "import torch\n",
        "print(f'CUDA: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available(): print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "else: raise RuntimeError('Enable GPU in Runtime > Change runtime type!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fe8733a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe8733a6",
        "outputId": "4fc4dd32-6d99-4e90-d9c5-789ad602cf09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hPackages installed!\n"
          ]
        }
      ],
      "source": [
        "# 2. Install packages\n",
        "!pip install -q nltk gensim pillow tqdm\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "print('Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6fa989f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fa989f7",
        "outputId": "f78d7923-f3c6-4f8e-de59-6eafdc4c5a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 3. Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5f690f15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f690f15",
        "outputId": "e1a2dbf9-44d5-470d-aff8-222653b7343f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data copied!\n"
          ]
        }
      ],
      "source": [
        "# 4. Copy data (only processed data needed, not raw WikiArt)\n",
        "import os\n",
        "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
        "!mkdir -p /content/artemis\n",
        "!cp -r \"{DRIVE_PATH}/data\" /content/artemis/\n",
        "!cp -r \"{DRIVE_PATH}/utils\" /content/artemis/\n",
        "!cp -r \"{DRIVE_PATH}/models\" /content/artemis/\n",
        "!cp \"{DRIVE_PATH}/train.py\" /content/artemis/\n",
        "print('Data copied!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4b. CRITICAL FIX: Patch cnn_lstm.py to fix contiguous tensor issue\n",
        "# This fixes \"rnn: hx is not contiguous\" error\n",
        "import re\n",
        "\n",
        "cnn_file = '/content/artemis/models/cnn_lstm.py'\n",
        "with open(cnn_file, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Fix the non-contiguous hidden state issue\n",
        "old_pattern = r'\\(h\\[:, :batch_size_t\\], c\\[:, :batch_size_t\\]\\)'\n",
        "new_pattern = '(h[:, :batch_size_t].contiguous(), c[:, :batch_size_t].contiguous())'\n",
        "\n",
        "if '.contiguous()' not in content:\n",
        "    content = re.sub(old_pattern, new_pattern, content)\n",
        "    with open(cnn_file, 'w') as f:\n",
        "        f.write(content)\n",
        "    print('✓ Patched cnn_lstm.py with contiguous() fix')\n",
        "else:\n",
        "    print('✓ cnn_lstm.py already has contiguous() fix')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imsEtRMoSwRD",
        "outputId": "cefd7eb7-9643-4718-9605-5fcf8effcbf0"
      },
      "id": "imsEtRMoSwRD",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ cnn_lstm.py already has contiguous() fix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9fd5e86d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fd5e86d",
        "outputId": "2e544a6d-bd86-48b5-a7c5-9ea9b9564e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working directory: /content/artemis\n"
          ]
        }
      ],
      "source": [
        "# 5. Setup paths\n",
        "import sys\n",
        "os.chdir('/content/artemis')\n",
        "sys.path.insert(0, '/content/artemis')\n",
        "print(f'Working directory: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this in Colab to diagnose the filename encoding issue\n",
        "# Paste this into a cell and run it\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Check what's in the JSON\n",
        "splits_dir = Path('/content/artemis/data/processed/splits')\n",
        "images_dir = Path('/content/artemis/data/processed/images')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DIAGNOSING FILENAME ENCODING ISSUE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. Sample filenames from JSON\n",
        "print(\"\\n1. SAMPLE NAMES FROM JSON (train.json):\")\n",
        "with open(splits_dir / 'train.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Find some with special characters\n",
        "special_char_paintings = []\n",
        "for p in data['paintings']:\n",
        "    name = p['painting']\n",
        "    if any(ord(c) > 127 for c in name):\n",
        "        special_char_paintings.append((p['style'], name))\n",
        "        if len(special_char_paintings) >= 5:\n",
        "            break\n",
        "\n",
        "for style, name in special_char_paintings:\n",
        "    print(f\"  JSON: {style}/{name}.jpg\")\n",
        "    # Check if file exists\n",
        "    path = images_dir / style / f\"{name}.jpg\"\n",
        "    print(f\"  Exists: {path.exists()}\")\n",
        "\n",
        "# 2. Sample actual filenames from disk\n",
        "print(\"\\n2. SAMPLE ACTUAL FILENAMES ON DISK:\")\n",
        "for style_dir in sorted(images_dir.iterdir())[:3]:\n",
        "    if style_dir.is_dir():\n",
        "        files = list(style_dir.glob('*.jpg'))[:3]\n",
        "        for f in files:\n",
        "            name = f.name\n",
        "            # Show hex of any special chars\n",
        "            if any(ord(c) > 127 for c in name):\n",
        "                print(f\"  DISK: {style_dir.name}/{name}\")\n",
        "                print(f\"        Bytes: {name.encode('utf-8')[:50]}\")\n",
        "\n",
        "# 3. Find mismatches for one problematic artist\n",
        "print(\"\\n3. CHECKING JOAQUÍN SOROLLA FILES:\")\n",
        "target_style = \"Impressionism\"\n",
        "target_prefix = \"joaqu\"\n",
        "\n",
        "# What's in JSON?\n",
        "json_names = []\n",
        "for p in data['paintings']:\n",
        "    if p['style'] == target_style and target_prefix in p['painting'].lower():\n",
        "        json_names.append(p['painting'])\n",
        "\n",
        "print(f\"  Found {len(json_names)} in JSON with '{target_prefix}'\")\n",
        "if json_names:\n",
        "    print(f\"  Example: {json_names[0]}\")\n",
        "    print(f\"  Bytes: {json_names[0].encode('utf-8')}\")\n",
        "\n",
        "# What's on disk?\n",
        "disk_names = []\n",
        "style_path = images_dir / target_style\n",
        "if style_path.exists():\n",
        "    for f in style_path.glob('*.jpg'):\n",
        "        if target_prefix in f.stem.lower():\n",
        "            disk_names.append(f.stem)\n",
        "\n",
        "print(f\"  Found {len(disk_names)} on disk with '{target_prefix}'\")\n",
        "if disk_names:\n",
        "    print(f\"  Example: {disk_names[0]}\")\n",
        "    print(f\"  Bytes: {disk_names[0].encode('utf-8')}\")\n",
        "\n",
        "# 4. Compare byte representations\n",
        "print(\"\\n4. BYTE COMPARISON:\")\n",
        "if json_names and disk_names:\n",
        "    j = json_names[0]\n",
        "    d = disk_names[0]\n",
        "    print(f\"  JSON name bytes: {j.encode('utf-8')}\")\n",
        "    print(f\"  DISK name bytes: {d.encode('utf-8')}\")\n",
        "    print(f\"  Are they equal? {j == d}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqARoM0xaZAY",
        "outputId": "39f22722-ac73-4697-c27d-4f3f71498e0f"
      },
      "id": "sqARoM0xaZAY",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DIAGNOSING FILENAME ENCODING ISSUE\n",
            "======================================================================\n",
            "\n",
            "1. SAMPLE NAMES FROM JSON (train.json):\n",
            "  JSON: Impressionism/joaquã­n-sorolla_the-tunny-catch-1919.jpg\n",
            "  Exists: False\n",
            "  JSON: Impressionism/joaquã­n-sorolla_children-on-the-seashore-1903.jpg\n",
            "  Exists: False\n",
            "  JSON: Impressionism/joaquã­n-sorolla_square-of-valencia.jpg\n",
            "  Exists: False\n",
            "  JSON: Impressionism/joaquã­n-sorolla_on-the-beach-at-valencia-1910.jpg\n",
            "  Exists: False\n",
            "  JSON: Romanticism/arnold-bã¶cklin_portrait-of-angela-b-cklin-in-red-fishnet.jpg\n",
            "  Exists: False\n",
            "\n",
            "2. SAMPLE ACTUAL FILENAMES ON DISK:\n",
            "\n",
            "3. CHECKING JOAQUÍN SOROLLA FILES:\n",
            "  Found 62 in JSON with 'joaqu'\n",
            "  Example: joaquã­n-sorolla_the-tunny-catch-1919\n",
            "  Bytes: b'joaqu\\xc3\\xa3\\xc2\\xadn-sorolla_the-tunny-catch-1919'\n",
            "  Found 81 on disk with 'joaqu'\n",
            "  Example: joaquã­n-sorolla_study-for-the-comeback-of-the-fisheries-1894\n",
            "  Bytes: b'joaqua\\xcc\\x83\\xc2\\xadn-sorolla_study-for-the-comeback-of-the-fisheries-1894'\n",
            "\n",
            "4. BYTE COMPARISON:\n",
            "  JSON name bytes: b'joaqu\\xc3\\xa3\\xc2\\xadn-sorolla_the-tunny-catch-1919'\n",
            "  DISK name bytes: b'joaqua\\xcc\\x83\\xc2\\xadn-sorolla_study-for-the-comeback-of-the-fisheries-1894'\n",
            "  Are they equal? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "34016e37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34016e37",
        "outputId": "315c36da-c783-463b-cffc-ae44fee9147f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports OK (with forced reload)!\n"
          ]
        }
      ],
      "source": [
        "# 6. Imports (with forced reload to ensure fresh code)\n",
        "import torch, json, time, gc\n",
        "import numpy as np\n",
        "import importlib\n",
        "\n",
        "# Import modules\n",
        "from utils import text_preprocessing, data_loader\n",
        "from models import cnn_lstm, vision_transformer\n",
        "import train\n",
        "\n",
        "# Force reload to use the updated code (not cached versions)\n",
        "importlib.reload(cnn_lstm)\n",
        "importlib.reload(vision_transformer)\n",
        "importlib.reload(train)\n",
        "\n",
        "# Now import the classes/functions we need\n",
        "from utils.text_preprocessing import TextPreprocessor\n",
        "from utils.data_loader import create_data_loaders\n",
        "from models.cnn_lstm import create_model as create_cnn_model\n",
        "from models.vision_transformer import VisionTransformerCaptioning\n",
        "from train import Trainer\n",
        "\n",
        "print('All imports OK (with forced reload)!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4799976e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4799976e",
        "outputId": "0d84209f-f176-4ff5-e67f-7afe4913a6d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configurations defined:\n",
            "  1: colab_cnn_high_lr - CNN+LSTM High LR - Best from training analysis\n",
            "  2: colab_cnn_glove - CNN+LSTM with GloVe Embeddings\n",
            "  3: colab_vit_standard - Vision Transformer Standard (6 layers)\n",
            "  4: colab_vit_compact - Vision Transformer Compact (4 layers, higher LR)\n"
          ]
        }
      ],
      "source": [
        "# 7. Define all 4 training configurations\n",
        "CONFIGS = {\n",
        "    1: {\n",
        "        'name': 'colab_cnn_high_lr',\n",
        "        'model_type': 'cnn_lstm',\n",
        "        'description': 'CNN+LSTM High LR - Best from training analysis',\n",
        "        'batch_size': 32,\n",
        "        'num_images': 15000,\n",
        "        'epochs': 30,\n",
        "        'learning_rate': 3e-4,\n",
        "        'embed_dim': 512,\n",
        "        'hidden_dim': 1024,\n",
        "        'attention_dim': 512,\n",
        "        'dropout': 0.4,\n",
        "    },\n",
        "    2: {\n",
        "        'name': 'colab_cnn_glove',\n",
        "        'model_type': 'cnn_lstm',\n",
        "        'description': 'CNN+LSTM with GloVe Embeddings',\n",
        "        'batch_size': 32,\n",
        "        'num_images': 15000,\n",
        "        'epochs': 30,\n",
        "        'learning_rate': 2e-4,\n",
        "        'embed_dim': 300,  # GloVe dimension\n",
        "        'hidden_dim': 512,\n",
        "        'attention_dim': 256,\n",
        "        'dropout': 0.3,\n",
        "        'use_glove': True,\n",
        "    },\n",
        "    3: {\n",
        "        'name': 'colab_vit_standard',\n",
        "        'model_type': 'vit',\n",
        "        'description': 'Vision Transformer Standard (6 layers)',\n",
        "        'batch_size': 32,\n",
        "        'num_images': 15000,\n",
        "        'epochs': 25,\n",
        "        'learning_rate': 2e-4,\n",
        "        'embed_dim': 256,\n",
        "        'num_heads': 8,\n",
        "        'num_layers': 6,\n",
        "        'ff_dim': 1024,\n",
        "        'dropout': 0.1,\n",
        "    },\n",
        "    4: {\n",
        "        'name': 'colab_vit_compact',\n",
        "        'model_type': 'vit',\n",
        "        'description': 'Vision Transformer Compact (4 layers, higher LR)',\n",
        "        'batch_size': 32,\n",
        "        'num_images': 15000,\n",
        "        'epochs': 25,\n",
        "        'learning_rate': 3e-4,\n",
        "        'embed_dim': 256,\n",
        "        'num_heads': 8,\n",
        "        'num_layers': 4,\n",
        "        'ff_dim': 512,\n",
        "        'dropout': 0.1,\n",
        "    },\n",
        "}\n",
        "\n",
        "print('Configurations defined:')\n",
        "for k, v in CONFIGS.items():\n",
        "    print(f\"  {k}: {v['name']} - {v['description']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "66a2bbc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66a2bbc3",
        "outputId": "a240fdb0-101a-46f8-a4e7-b62bd49cb883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LimitedLoader defined\n"
          ]
        }
      ],
      "source": [
        "# 8. Helper class for limiting batches\n",
        "class LimitedLoader:\n",
        "    def __init__(self, loader, max_batches):\n",
        "        self.loader = loader\n",
        "        self.max_batches = max_batches\n",
        "        self.batch_size = loader.batch_size\n",
        "        self.dataset = loader.dataset\n",
        "    def __iter__(self):\n",
        "        for i, b in enumerate(self.loader):\n",
        "            if i >= self.max_batches: break\n",
        "            yield b\n",
        "    def __len__(self): return min(len(self.loader), self.max_batches)\n",
        "\n",
        "print('LimitedLoader defined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "e51cc5b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e51cc5b3",
        "outputId": "acdeaae1-f882-46fd-f71e-5c522da3d539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Vocabulary loaded from: data/processed/vocabulary.json\n",
            "  - Vocabulary size: 10000\n",
            "  - Max caption length: 30\n",
            "Vocabulary size: 10000\n",
            "GloVe embeddings loaded: torch.Size([10000, 300])\n"
          ]
        }
      ],
      "source": [
        "# 9. Load vocabulary (shared across all models)\n",
        "text_proc = TextPreprocessor()\n",
        "text_proc.load_vocabulary('data/processed/vocabulary.json')\n",
        "print(f'Vocabulary size: {text_proc.vocab_size}')\n",
        "\n",
        "# Load GloVe embeddings for config 2 (convert to tensor)\n",
        "glove_matrix = None\n",
        "if os.path.exists('data/embeddings/glove_embeddings.npy'):\n",
        "    glove_np = np.load('data/embeddings/glove_embeddings.npy')\n",
        "    glove_matrix = torch.tensor(glove_np, dtype=torch.float32)\n",
        "    print(f'GloVe embeddings loaded: {glove_matrix.shape}')\n",
        "else:\n",
        "    print('Warning: GloVe embeddings not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "a0f1dbec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0f1dbec",
        "outputId": "a8ddb6ca-9278-4851-f083-3a481da23884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training function defined\n"
          ]
        }
      ],
      "source": [
        "# 10. Training function\n",
        "def train_config(config_num, text_proc, glove_matrix=None):\n",
        "    \"\"\"Train a single configuration.\"\"\"\n",
        "    config = CONFIGS[config_num]\n",
        "    name = config['name']\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"STARTING CONFIG {config_num}: {name}\")\n",
        "    print(f\"Description: {config['description']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    loaders = create_data_loaders(\n",
        "        text_preprocessor=text_proc,\n",
        "        batch_size=config['batch_size'],\n",
        "        num_workers=2,\n",
        "        splits=['train', 'val']\n",
        "    )\n",
        "    train_loader = LimitedLoader(loaders['train'], config['num_images'] // config['batch_size'])\n",
        "    val_loader = LimitedLoader(loaders['val'], 50)\n",
        "    print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n",
        "\n",
        "    # Create model\n",
        "    if config['model_type'] == 'cnn_lstm':\n",
        "        if config.get('use_glove') and glove_matrix is not None:\n",
        "            model = create_cnn_model(\n",
        "                embedding_matrix=glove_matrix,\n",
        "                vocab_size=text_proc.vocab_size,\n",
        "                embed_dim=config['embed_dim'],\n",
        "                decoder_dim=config['hidden_dim'],\n",
        "                attention_dim=config['attention_dim'],\n",
        "                dropout=config['dropout']\n",
        "            )\n",
        "        else:\n",
        "            model = create_cnn_model(\n",
        "                vocab_size=text_proc.vocab_size,\n",
        "                embed_dim=config['embed_dim'],\n",
        "                decoder_dim=config['hidden_dim'],\n",
        "                attention_dim=config['attention_dim'],\n",
        "                dropout=config['dropout']\n",
        "            )\n",
        "    else:  # vit\n",
        "        model = VisionTransformerCaptioning(\n",
        "            vocab_size=text_proc.vocab_size,\n",
        "            embed_dim=config['embed_dim'],\n",
        "            num_heads=config['num_heads'],\n",
        "            encoder_layers=config['num_layers'],\n",
        "            decoder_layers=config['num_layers'],\n",
        "            mlp_ratio=4,\n",
        "            max_length=30,\n",
        "            dropout=config['dropout'],\n",
        "            img_size=128,\n",
        "            patch_size=16\n",
        "        )\n",
        "\n",
        "    model = model.to('cuda')\n",
        "    print(f'Model params: {sum(p.numel() for p in model.parameters()):,}')\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(f\"checkpoints/{name}\", exist_ok=True)\n",
        "    os.makedirs(f\"outputs/{name}\", exist_ok=True)\n",
        "\n",
        "    # Train\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        text_preprocessor=text_proc,\n",
        "        learning_rate=config['learning_rate'],\n",
        "        device='cuda',\n",
        "        checkpoint_dir=f\"checkpoints/{name}\",\n",
        "        output_dir=f\"outputs/{name}\"\n",
        "    )\n",
        "\n",
        "    t0 = time.time()\n",
        "    history = trainer.train(num_epochs=config['epochs'])\n",
        "    duration = (time.time() - t0) / 60\n",
        "\n",
        "    # Results\n",
        "    best_bleu = max(history['val_bleu'])\n",
        "    best_loss = min(history['val_loss'])\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"COMPLETED: {name}\")\n",
        "    print(f\"Duration: {duration:.1f} min\")\n",
        "    print(f\"Best BLEU: {best_bleu:.4f}\")\n",
        "    print(f\"Best Loss: {best_loss:.4f}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Save to Drive immediately\n",
        "    DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
        "    os.system(f'mkdir -p \"{DRIVE_PATH}/checkpoints\"')\n",
        "    os.system(f'mkdir -p \"{DRIVE_PATH}/outputs\"')\n",
        "    os.system(f'cp -r \"checkpoints/{name}\" \"{DRIVE_PATH}/checkpoints/\"')\n",
        "    os.system(f'cp -r \"outputs/{name}\" \"{DRIVE_PATH}/outputs/\"')\n",
        "    print(f'Saved to Drive: {DRIVE_PATH}')\n",
        "\n",
        "    # Cleanup GPU memory\n",
        "    del model, trainer, train_loader, val_loader, loaders\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {'name': name, 'duration': duration, 'best_bleu': best_bleu, 'best_loss': best_loss}\n",
        "\n",
        "print('Training function defined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ce7b3246",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce7b3246",
        "outputId": "3794d538-84a9-4d59-e0df-0ab6e475b53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING CONFIG 1: colab_cnn_high_lr\n",
            "Description: CNN+LSTM High LR - Best from training analysis\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "CREATING DATA LOADERS\n",
            "======================================================================\n",
            "\n",
            "TRAIN DataLoader:\n",
            "  - Loaded 68108 image-caption pairs from train.json\n",
            "  - Samples: 68,108\n",
            "  - Paintings: 11,991\n",
            "  - Batches: 2,128\n",
            "  - Batch size: 32\n",
            "  - Shuffle: True\n",
            "\n",
            "VAL DataLoader:\n",
            "  - Loaded 8575 image-caption pairs from val.json\n",
            "  - Samples: 8,575\n",
            "  - Paintings: 1,490\n",
            "  - Batches: 268\n",
            "  - Batch size: 32\n",
            "  - Shuffle: False\n",
            "Train batches: 468, Val batches: 50\n",
            "Model params: 43,869,009\n",
            "\n",
            "======================================================================\n",
            "COMPLETED: colab_cnn_high_lr\n",
            "Duration: 65.8 min\n",
            "Best BLEU: 0.0197\n",
            "Best Loss: 3.8454\n",
            "======================================================================\n",
            "Saved to Drive: /content/drive/MyDrive/artemis-captioning\n",
            "\n",
            "======================================================================\n",
            "STARTING CONFIG 2: colab_cnn_glove\n",
            "Description: CNN+LSTM with GloVe Embeddings\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "CREATING DATA LOADERS\n",
            "======================================================================\n",
            "\n",
            "TRAIN DataLoader:\n",
            "  - Loaded 68108 image-caption pairs from train.json\n",
            "  - Samples: 68,108\n",
            "  - Paintings: 11,991\n",
            "  - Batches: 2,128\n",
            "  - Batch size: 32\n",
            "  - Shuffle: True\n",
            "\n",
            "VAL DataLoader:\n",
            "  - Loaded 8575 image-caption pairs from val.json\n",
            "  - Samples: 8,575\n",
            "  - Paintings: 1,490\n",
            "  - Batches: 268\n",
            "  - Batch size: 32\n",
            "  - Shuffle: False\n",
            "Train batches: 468, Val batches: 50\n",
            "Model params: 24,322,833\n",
            "\n",
            "======================================================================\n",
            "COMPLETED: colab_cnn_glove\n",
            "Duration: 53.3 min\n",
            "Best BLEU: 0.0122\n",
            "Best Loss: 3.9042\n",
            "======================================================================\n",
            "Saved to Drive: /content/drive/MyDrive/artemis-captioning\n",
            "\n",
            "======================================================================\n",
            "STARTING CONFIG 3: colab_vit_standard\n",
            "Description: Vision Transformer Standard (6 layers)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "CREATING DATA LOADERS\n",
            "======================================================================\n",
            "\n",
            "TRAIN DataLoader:\n",
            "  - Loaded 68108 image-caption pairs from train.json\n",
            "  - Samples: 68,108\n",
            "  - Paintings: 11,991\n",
            "  - Batches: 2,128\n",
            "  - Batch size: 32\n",
            "  - Shuffle: True\n",
            "\n",
            "VAL DataLoader:\n",
            "  - Loaded 8575 image-caption pairs from val.json\n",
            "  - Samples: 8,575\n",
            "  - Paintings: 1,490\n",
            "  - Batches: 268\n",
            "  - Batch size: 32\n",
            "  - Shuffle: False\n",
            "Train batches: 468, Val batches: 50\n",
            "Model params: 16,403,984\n",
            "\n",
            "======================================================================\n",
            "COMPLETED: colab_vit_standard\n",
            "Duration: 23.6 min\n",
            "Best BLEU: 0.0180\n",
            "Best Loss: 3.9168\n",
            "======================================================================\n",
            "Saved to Drive: /content/drive/MyDrive/artemis-captioning\n",
            "\n",
            "======================================================================\n",
            "STARTING CONFIG 4: colab_vit_compact\n",
            "Description: Vision Transformer Compact (4 layers, higher LR)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "CREATING DATA LOADERS\n",
            "======================================================================\n",
            "\n",
            "TRAIN DataLoader:\n",
            "  - Loaded 68108 image-caption pairs from train.json\n",
            "  - Samples: 68,108\n",
            "  - Paintings: 11,991\n",
            "  - Batches: 2,128\n",
            "  - Batch size: 32\n",
            "  - Shuffle: True\n",
            "\n",
            "VAL DataLoader:\n",
            "  - Loaded 8575 image-caption pairs from val.json\n",
            "  - Samples: 8,575\n",
            "  - Paintings: 1,490\n",
            "  - Batches: 268\n",
            "  - Batch size: 32\n",
            "  - Shuffle: False\n",
            "Train batches: 468, Val batches: 50\n",
            "Model params: 12,717,584\n",
            "\n",
            "======================================================================\n",
            "COMPLETED: colab_vit_compact\n",
            "Duration: 18.0 min\n",
            "Best BLEU: 0.0175\n",
            "Best Loss: 3.9327\n",
            "======================================================================\n",
            "Saved to Drive: /content/drive/MyDrive/artemis-captioning\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ALL TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Total time: 162.9 minutes (2.7 hours)\n",
            "\n",
            "Results Summary:\n",
            "--------------------------------------------------\n",
            "  colab_cnn_high_lr: BLEU=0.0197, Loss=3.8454, Time=65.8min\n",
            "  colab_cnn_glove: BLEU=0.0122, Loss=3.9042, Time=53.3min\n",
            "  colab_vit_standard: BLEU=0.0180, Loss=3.9168, Time=23.6min\n",
            "  colab_vit_compact: BLEU=0.0175, Loss=3.9327, Time=18.0min\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 11. RUN ALL 4 CONFIGURATIONS SEQUENTIALLY\n",
        "all_results = []\n",
        "total_start = time.time()\n",
        "\n",
        "for config_num in [1, 2, 3, 4]:\n",
        "    try:\n",
        "        result = train_config(config_num, text_proc, glove_matrix)\n",
        "        all_results.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in config {config_num}: {e}\")\n",
        "        all_results.append({'name': CONFIGS[config_num]['name'], 'error': str(e)})\n",
        "\n",
        "total_duration = (time.time() - total_start) / 60\n",
        "\n",
        "print(f\"\\n\\n{'='*70}\")\n",
        "print(\"ALL TRAINING COMPLETE!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total time: {total_duration:.1f} minutes ({total_duration/60:.1f} hours)\")\n",
        "print(\"\\nResults Summary:\")\n",
        "print(\"-\" * 50)\n",
        "for r in all_results:\n",
        "    if 'error' in r:\n",
        "        print(f\"  {r['name']}: ERROR - {r['error']}\")\n",
        "    else:\n",
        "        print(f\"  {r['name']}: BLEU={r['best_bleu']:.4f}, Loss={r['best_loss']:.4f}, Time={r['duration']:.1f}min\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "116d9915",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "116d9915",
        "outputId": "d73f612d-0c01-47e1-d7fc-305ec4c6cc9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary saved to /content/drive/MyDrive/artemis-captioning/colab_training_summary.json\n"
          ]
        }
      ],
      "source": [
        "# 12. Save final summary to Drive\n",
        "import json\n",
        "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
        "\n",
        "summary = {\n",
        "    'total_duration_minutes': total_duration,\n",
        "    'results': all_results,\n",
        "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "with open(f'{DRIVE_PATH}/colab_training_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"Summary saved to {DRIVE_PATH}/colab_training_summary.json\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}