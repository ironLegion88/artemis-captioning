{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check GPU\n",
    "import torch\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available(): print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "else: raise RuntimeError('Enable GPU!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install packages\n",
    "!pip install -q nltk gensim pillow tqdm\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Copy data\n",
    "import os\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "!mkdir -p /content/artemis\n",
    "!cp -r \"{DRIVE_PATH}/data\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/utils\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/models\" /content/artemis/\n",
    "!cp \"{DRIVE_PATH}/train.py\" /content/artemis/\n",
    "print('Copied!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd255dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Setup path\n",
    "import sys\n",
    "os.chdir('/content/artemis')\n",
    "sys.path.insert(0, '/content/artemis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Imports\n",
    "import torch, json, time\n",
    "from utils.text_preprocessing import TextPreprocessor\n",
    "from utils.data_loader import create_data_loaders\n",
    "from models.cnn_lstm import create_model\n",
    "from train import Trainer\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431faf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Config\n",
    "CONFIG = {\n",
    "    'name': 'colab_cnn_large',\n",
    "    'batch_size': 32,\n",
    "    'num_images': 15000,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'embed_dim': 512,\n",
    "    'hidden_dim': 1024,\n",
    "    'attention_dim': 512,\n",
    "    'dropout': 0.4,\n",
    "}\n",
    "print(f\"Training: {CONFIG['name']}\")\n",
    "print(f\"Images: ~{CONFIG['num_images']}, Epochs: {CONFIG['epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Load vocabulary\n",
    "text_proc = TextPreprocessor()\n",
    "text_proc.load_vocabulary('data/processed/vocabulary.json')\n",
    "print(f'Vocab: {text_proc.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Data loaders\n",
    "class LimitedLoader:\n",
    "    def __init__(self, loader, max_batches):\n",
    "        self.loader = loader\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_size = loader.batch_size\n",
    "        self.dataset = loader.dataset\n",
    "    def __iter__(self):\n",
    "        for i, b in enumerate(self.loader):\n",
    "            if i >= self.max_batches: break\n",
    "            yield b\n",
    "    def __len__(self): return min(len(self.loader), self.max_batches)\n",
    "\n",
    "loaders = create_data_loaders(text_preprocessor=text_proc, batch_size=CONFIG['batch_size'], num_workers=2, splits=['train','val'])\n",
    "train_loader = LimitedLoader(loaders['train'], CONFIG['num_images']//CONFIG['batch_size'])\n",
    "val_loader = LimitedLoader(loaders['val'], 50)\n",
    "print(f'Train: {len(train_loader)} batches, Val: {len(val_loader)} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c736e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create model\n",
    "model = create_model(\n",
    "    vocab_size=text_proc.vocab_size,\n",
    "    embed_dim=CONFIG['embed_dim'],\n",
    "    decoder_dim=CONFIG['hidden_dim'],\n",
    "    attention_dim=CONFIG['attention_dim'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to('cuda')\n",
    "print(f'Params: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5dac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Train\n",
    "os.makedirs(f\"checkpoints/{CONFIG['name']}\", exist_ok=True)\n",
    "os.makedirs(f\"outputs/{CONFIG['name']}\", exist_ok=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    text_preprocessor=text_proc,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    device='cuda',\n",
    "    checkpoint_dir=f\"checkpoints/{CONFIG['name']}\",\n",
    "    output_dir=f\"outputs/{CONFIG['name']}\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\\nSTARTING TRAINING: {CONFIG['name']}\\n{'='*70}\")\n",
    "t0 = time.time()\n",
    "history = trainer.train(num_epochs=CONFIG['epochs'])\n",
    "duration = (time.time()-t0)/60\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DONE! Duration: {duration:.1f} min\")\n",
    "print(f\"Best BLEU: {max(history['val_bleu']):.4f}\")\n",
    "print(f\"Best Loss: {min(history['val_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c110e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Save to Drive\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "!mkdir -p \"{DRIVE_PATH}/checkpoints\"\n",
    "!mkdir -p \"{DRIVE_PATH}/outputs\"\n",
    "!cp -r \"checkpoints/{CONFIG['name']}\" \"{DRIVE_PATH}/checkpoints/\"\n",
    "!cp -r \"outputs/{CONFIG['name']}\" \"{DRIVE_PATH}/outputs/\"\n",
    "print(f\"Saved to {DRIVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
