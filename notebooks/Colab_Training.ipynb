{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b77acb",
   "metadata": {},
   "source": [
    "## 1. Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5cbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q nltk gensim pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d3334",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive and Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd460c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths - MODIFY THIS to match your Drive folder\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your uploaded data in Google Drive\n",
    "DRIVE_DATA_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "\n",
    "# Check if data exists\n",
    "if os.path.exists(DRIVE_DATA_PATH):\n",
    "    print(f\"✓ Data folder found: {DRIVE_DATA_PATH}\")\n",
    "    print(\"Contents:\")\n",
    "    for item in os.listdir(DRIVE_DATA_PATH):\n",
    "        item_path = os.path.join(DRIVE_DATA_PATH, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  - {item}/ ({len(os.listdir(item_path))} items)\")\n",
    "        else:\n",
    "            print(f\"  - {item}\")\n",
    "else:\n",
    "    print(f\"✗ Data folder not found: {DRIVE_DATA_PATH}\")\n",
    "    print(\"Please upload your data to Google Drive first.\")\n",
    "    print(\"\\nRequired structure:\")\n",
    "    print(\"  artemis-captioning/\")\n",
    "    print(\"    ├── data/\")\n",
    "    print(\"    │   ├── processed/\")\n",
    "    print(\"    │   │   ├── images/      # Pre-resized 128x128 images (~57 MB)\")\n",
    "    print(\"    │   │   ├── splits/\")\n",
    "    print(\"    │   │   └── vocabulary.json\")\n",
    "    print(\"    │   └── raw/wikiart/     # Original images (if not using preprocessed)\")\n",
    "    print(\"    ├── models/\")\n",
    "    print(\"    ├── utils/\")\n",
    "    print(\"    ├── scripts/\")\n",
    "    print(\"    └── train.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e922950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to local storage for faster access\n",
    "!mkdir -p /content/artemis\n",
    "!cp -r \"{DRIVE_DATA_PATH}/data\" /content/artemis/\n",
    "!cp -r \"{DRIVE_DATA_PATH}/utils\" /content/artemis/\n",
    "!cp -r \"{DRIVE_DATA_PATH}/models\" /content/artemis/\n",
    "!cp -r \"{DRIVE_DATA_PATH}/scripts\" /content/artemis/\n",
    "!cp \"{DRIVE_DATA_PATH}/train.py\" /content/artemis/\n",
    "\n",
    "# Check if preprocessed images exist\n",
    "import os\n",
    "processed_images_path = \"/content/artemis/data/processed/images\"\n",
    "if os.path.exists(processed_images_path) and os.listdir(processed_images_path):\n",
    "    num_images = sum(1 for _ in Path(processed_images_path).rglob(\"*.jpg\"))\n",
    "    print(f\"✓ Found {num_images} preprocessed images (128x128)\")\n",
    "else:\n",
    "    print(\"⚠ Preprocessed images not found. Will use raw images with on-the-fly resizing.\")\n",
    "    print(\"  For faster training, run preprocess_images.py locally first.\")\n",
    "\n",
    "print(\"✓ Data copied to /content/artemis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a427d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory\n",
    "import sys\n",
    "os.chdir('/content/artemis')\n",
    "sys.path.insert(0, '/content/artemis')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1896da",
   "metadata": {},
   "source": [
    "## 3. Import Modules and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9413d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import project modules\n",
    "from utils.text_preprocessing import TextPreprocessor\n",
    "from utils.data_loader import create_data_loaders\n",
    "from models.cnn_lstm import create_model as create_cnn_lstm\n",
    "from models.vision_transformer import create_vit_model\n",
    "from train import Trainer\n",
    "\n",
    "print(\"✓ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81150f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 32  # Larger batch size for GPU\n",
    "NUM_WORKERS = 2\n",
    "MAX_BATCHES = 469  # ~15000 images (469 * 32 = 15008)\n",
    "EPOCHS = 50\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max batches: {MAX_BATCHES} (~{MAX_BATCHES * BATCH_SIZE} images)\")\n",
    "print(f\"Epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "text_proc = TextPreprocessor()\n",
    "text_proc.load_vocabulary('data/processed/vocabulary.json')\n",
    "print(f\"Vocabulary size: {text_proc.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16407fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "loaders = create_data_loaders(\n",
    "    text_preprocessor=text_proc,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    splits=['train', 'val']\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(loaders['train'].dataset)}\")\n",
    "print(f\"Val samples: {len(loaders['val'].dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e41fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited loader for controlled training\n",
    "class LimitedLoader:\n",
    "    def __init__(self, loader, max_batches):\n",
    "        self.loader = loader\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_size = loader.batch_size\n",
    "        self.dataset = loader.dataset\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i, batch in enumerate(self.loader):\n",
    "            if i >= self.max_batches:\n",
    "                break\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(len(self.loader), self.max_batches)\n",
    "\n",
    "# Create limited loaders\n",
    "train_loader = LimitedLoader(loaders['train'], MAX_BATCHES)\n",
    "val_loader = LimitedLoader(loaders['val'], MAX_BATCHES // 4)  # Smaller validation\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} (~{len(train_loader) * BATCH_SIZE} images)\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf87924",
   "metadata": {},
   "source": [
    "## 4. Train CNN+LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1785538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN+LSTM model\n",
    "cnn_lstm_model = create_cnn_lstm(\n",
    "    vocab_size=text_proc.vocab_size,\n",
    "    embed_dim=256\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in cnn_lstm_model.parameters())\n",
    "print(f\"CNN+LSTM Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bda61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "!mkdir -p checkpoints/colab_cnn_lstm\n",
    "!mkdir -p outputs/colab_cnn_lstm\n",
    "\n",
    "# Train CNN+LSTM\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING CNN+LSTM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cnn_trainer = Trainer(\n",
    "    model=cnn_lstm_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    text_preprocessor=text_proc,\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir='checkpoints/colab_cnn_lstm',\n",
    "    output_dir='outputs/colab_cnn_lstm'\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "cnn_history = cnn_trainer.train(num_epochs=EPOCHS)\n",
    "cnn_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nCNN+LSTM Training complete in {cnn_time/60:.1f} minutes\")\n",
    "print(f\"Best BLEU: {max(cnn_history['val_bleu']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41482c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CNN+LSTM training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(cnn_history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(cnn_history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('CNN+LSTM Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(cnn_history['val_bleu'], label='Val BLEU', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('BLEU Score')\n",
    "axes[1].set_title('CNN+LSTM BLEU Score')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/colab_cnn_lstm/training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ccdad",
   "metadata": {},
   "source": [
    "## 5. Train Vision Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de182fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vision Transformer model\n",
    "vit_model = create_vit_model(\n",
    "    vocab_size=text_proc.vocab_size,\n",
    "    embed_dim=256,\n",
    "    encoder_layers=6,  # More layers for GPU\n",
    "    decoder_layers=6\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in vit_model.parameters())\n",
    "print(f\"Vision Transformer Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "!mkdir -p checkpoints/colab_vit\n",
    "!mkdir -p outputs/colab_vit\n",
    "\n",
    "# Train ViT\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING VISION TRANSFORMER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "vit_trainer = Trainer(\n",
    "    model=vit_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    text_preprocessor=text_proc,\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir='checkpoints/colab_vit',\n",
    "    output_dir='outputs/colab_vit'\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "vit_history = vit_trainer.train(num_epochs=EPOCHS)\n",
    "vit_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nViT Training complete in {vit_time/60:.1f} minutes\")\n",
    "print(f\"Best BLEU: {max(vit_history['val_bleu']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965747e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ViT training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(vit_history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(vit_history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Vision Transformer Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(vit_history['val_bleu'], label='Val BLEU', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('BLEU Score')\n",
    "axes[1].set_title('Vision Transformer BLEU Score')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/colab_vit/training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6ec18",
   "metadata": {},
   "source": [
    "## 6. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b044f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison summary\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = {\n",
    "    'CNN+LSTM': {\n",
    "        'final_train_loss': cnn_history['train_loss'][-1],\n",
    "        'final_val_loss': cnn_history['val_loss'][-1],\n",
    "        'best_bleu': max(cnn_history['val_bleu']),\n",
    "        'training_time_min': cnn_time / 60\n",
    "    },\n",
    "    'ViT': {\n",
    "        'final_train_loss': vit_history['train_loss'][-1],\n",
    "        'final_val_loss': vit_history['val_loss'][-1],\n",
    "        'best_bleu': max(vit_history['val_bleu']),\n",
    "        'training_time_min': vit_time / 60\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name, metrics in comparison.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Final Train Loss: {metrics['final_train_loss']:.4f}\")\n",
    "    print(f\"  Final Val Loss: {metrics['final_val_loss']:.4f}\")\n",
    "    print(f\"  Best BLEU: {metrics['best_bleu']:.4f}\")\n",
    "    print(f\"  Training Time: {metrics['training_time_min']:.1f} minutes\")\n",
    "\n",
    "# Save comparison\n",
    "with open('outputs/model_comparison.json', 'w') as f:\n",
    "    json.dump(comparison, f, indent=2)\n",
    "print(\"\\n✓ Comparison saved to outputs/model_comparison.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cde01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(cnn_history['val_loss'], label='CNN+LSTM')\n",
    "axes[0].plot(vit_history['val_loss'], label='ViT')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].set_title('Validation Loss Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# BLEU comparison\n",
    "axes[1].plot(cnn_history['val_bleu'], label='CNN+LSTM')\n",
    "axes[1].plot(vit_history['val_bleu'], label='ViT')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('BLEU Score')\n",
    "axes[1].set_title('BLEU Score Comparison')\n",
    "axes[1].legend()\n",
    "\n",
    "# Bar chart\n",
    "models = ['CNN+LSTM', 'ViT']\n",
    "bleu_scores = [max(cnn_history['val_bleu']), max(vit_history['val_bleu'])]\n",
    "colors = ['steelblue', 'coral']\n",
    "axes[2].bar(models, bleu_scores, color=colors)\n",
    "axes[2].set_ylabel('Best BLEU Score')\n",
    "axes[2].set_title('Best BLEU Score Comparison')\n",
    "for i, v in enumerate(bleu_scores):\n",
    "    axes[2].text(i, v + 0.001, f'{v:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d98da3",
   "metadata": {},
   "source": [
    "## 7. Save Models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ec071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoints and outputs to Google Drive\n",
    "!mkdir -p \"{DRIVE_DATA_PATH}/checkpoints\"\n",
    "!mkdir -p \"{DRIVE_DATA_PATH}/outputs\"\n",
    "\n",
    "!cp -r checkpoints/colab_cnn_lstm \"{DRIVE_DATA_PATH}/checkpoints/\"\n",
    "!cp -r checkpoints/colab_vit \"{DRIVE_DATA_PATH}/checkpoints/\"\n",
    "!cp -r outputs/colab_cnn_lstm \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "!cp -r outputs/colab_vit \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "!cp outputs/model_comparison.json \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "!cp outputs/model_comparison.png \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "\n",
    "print(\"✓ Models and outputs saved to Google Drive!\")\n",
    "print(f\"Location: {DRIVE_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500dc2f",
   "metadata": {},
   "source": [
    "## 8. Test Caption Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c30d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best CNN+LSTM model and generate sample captions\n",
    "import random\n",
    "from PIL import Image\n",
    "from utils.image_preprocessing import ImagePreprocessor\n",
    "\n",
    "# Load a sample image from validation set\n",
    "with open('data/processed/splits/val.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# Pick random samples\n",
    "samples = random.sample(val_data, 5)\n",
    "\n",
    "# Image preprocessor\n",
    "img_proc = ImagePreprocessor()\n",
    "\n",
    "# Generate captions\n",
    "cnn_lstm_model.eval()\n",
    "\n",
    "print(\"Sample Caption Generation (CNN+LSTM):\\n\")\n",
    "for sample in samples:\n",
    "    img_path = f\"data/raw/wikiart/{sample['style']}/{sample['painting']}.jpg\"\n",
    "    if os.path.exists(img_path):\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = img_proc.val_transform(image).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # Generate caption\n",
    "        with torch.no_grad():\n",
    "            caption, _ = cnn_lstm_model.generate_caption(img_tensor)\n",
    "        \n",
    "        # Decode caption\n",
    "        generated = text_proc.decode(caption.cpu().numpy().tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Image: {sample['painting']}\")\n",
    "        print(f\"Ground Truth: {sample['utterance']}\")\n",
    "        print(f\"Generated: {generated}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5483815",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your trained models are saved to Google Drive. To use them:\n",
    "1. Download the checkpoints from `{DRIVE_DATA_PATH}/checkpoints/`\n",
    "2. Use `scripts/predict.py` on your local machine"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
