{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56dcd467",
   "metadata": {},
   "source": [
    "# ArtEmis Image Captioning - Multi-Model Training (Colab T4 GPU)\n",
    "\n",
    "## ðŸ“‹ SETUP CHECKLIST (Complete Before Running)\n",
    "\n",
    "**Before running this notebook:**\n",
    "\n",
    "1. **Enable GPU Runtime:**\n",
    "   - Go to `Runtime` â†’ `Change runtime type` â†’ Select `T4 GPU`\n",
    "\n",
    "2. **Upload Data to Google Drive:**\n",
    "   - Create folder: `Google Drive/artemis-captioning/`\n",
    "   - Upload these from your local machine:\n",
    "     ```\n",
    "     artemis-captioning/\n",
    "     â”œâ”€â”€ data/\n",
    "     â”‚   â”œâ”€â”€ processed/\n",
    "     â”‚   â”‚   â”œâ”€â”€ images/        (5000 pre-resized 128x128 images, ~57 MB)\n",
    "     â”‚   â”‚   â”œâ”€â”€ splits/        (train.json, val.json, test.json)\n",
    "     â”‚   â”‚   â”œâ”€â”€ captions/      (caption JSON files)\n",
    "     â”‚   â”‚   â””â”€â”€ vocabulary.json\n",
    "     â”‚   â””â”€â”€ embeddings/        (optional - glove, word2vec, tfidf)\n",
    "     â”œâ”€â”€ utils/                 (all Python files)\n",
    "     â”œâ”€â”€ models/                (all Python files)\n",
    "     â””â”€â”€ train.py\n",
    "     ```\n",
    "\n",
    "3. **Update DRIVE_DATA_PATH** in Cell 6 if needed (default: `/content/drive/MyDrive/artemis-captioning`)\n",
    "\n",
    "## ðŸš€ Training Configurations\n",
    "\n",
    "| Config | Model | Images | Epochs | Est. Time |\n",
    "|--------|-------|--------|--------|-----------|\n",
    "| colab_cnn_large | CNN+LSTM (512 embed, 1024 hidden) | 15,000 | 50 | ~3-4 hours |\n",
    "| colab_vit_standard | ViT (256 embed, 6 layers) | 15,000 | 50 | ~2-3 hours |\n",
    "| colab_cnn_glove | CNN+LSTM + GloVe embeddings | 15,000 | 40 | ~2-3 hours |\n",
    "\n",
    "**Total estimated time: ~8-10 hours** (run overnight or train one at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available(): print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nltk gensim pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e9d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a370cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "for f in ['data/processed/vocabulary.json', 'train.py', 'models/cnn_lstm.py']:\n",
    "    print('V' if os.path.exists(f'{DRIVE_PATH}/{f}') else 'X', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/artemis\n",
    "!cp -r \"{DRIVE_PATH}/data\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/utils\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/models\" /content/artemis/\n",
    "!cp \"{DRIVE_PATH}/train.py\" /content/artemis/\n",
    "print('Copied to /content/artemis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d40ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "os.chdir('/content/artemis')\n",
    "sys.path.insert(0, '/content/artemis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b514d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, time, numpy as np\n",
    "from utils.text_preprocessing import TextPreprocessor\n",
    "from utils.data_loader import create_data_loaders\n",
    "from models.cnn_lstm import create_model as create_cnn_model\n",
    "from models.vision_transformer import VisionTransformerCaptioning\n",
    "from train import Trainer\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4da742",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "CONFIGS = {\n",
    "    'colab_cnn_large': {\n",
    "        'model_type': 'cnn_lstm', 'batch_size': 32, 'num_images': 15000,\n",
    "        'epochs': 50, 'learning_rate': 1e-4, 'embed_dim': 512,\n",
    "        'hidden_dim': 1024, 'attention_dim': 512, 'dropout': 0.4,\n",
    "    },\n",
    "    'colab_vit_standard': {\n",
    "        'model_type': 'vit', 'batch_size': 32, 'num_images': 15000,\n",
    "        'epochs': 50, 'learning_rate': 1e-4, 'embed_dim': 256,\n",
    "        'num_heads': 8, 'num_layers': 6, 'ff_dim': 1024, 'dropout': 0.1,\n",
    "    },\n",
    "    'colab_cnn_glove': {\n",
    "        'model_type': 'cnn_lstm', 'batch_size': 32, 'num_images': 15000,\n",
    "        'epochs': 40, 'learning_rate': 5e-5, 'embed_dim': 300,\n",
    "        'hidden_dim': 512, 'attention_dim': 256, 'dropout': 0.3, 'use_glove': True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_proc = TextPreprocessor()\n",
    "text_proc.load_vocabulary('data/processed/vocabulary.json')\n",
    "vocab_size = text_proc.vocab_size\n",
    "print(f'Vocab: {vocab_size}')\n",
    "\n",
    "glove = None\n",
    "if os.path.exists('data/embeddings/glove_embeddings.npy'):\n",
    "    glove = np.load('data/embeddings/glove_embeddings.npy')\n",
    "    print(f'GloVe: {glove.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c14a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitedLoader:\n",
    "    def __init__(self, loader, max_batches):\n",
    "        self.loader = loader\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_size = loader.batch_size\n",
    "        self.dataset = loader.dataset\n",
    "    def __iter__(self):\n",
    "        for i, b in enumerate(self.loader):\n",
    "            if i >= self.max_batches: break\n",
    "            yield b\n",
    "    def __len__(self): return min(len(self.loader), self.max_batches)\n",
    "\n",
    "def train_model(name, cfg):\n",
    "    print(f'\\n{\"=\"*70}\\nTRAINING: {name}\\n{\"=\"*70}')\n",
    "    ckpt_dir, out_dir = f'checkpoints/{name}', f'outputs/{name}'\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    loaders = create_data_loaders(text_preprocessor=text_proc, batch_size=cfg['batch_size'], num_workers=NUM_WORKERS, splits=['train','val'])\n",
    "    train_loader = LimitedLoader(loaders['train'], cfg['num_images']//cfg['batch_size'])\n",
    "    val_loader = LimitedLoader(loaders['val'], 50)\n",
    "    print(f'Batches: train={len(train_loader)}, val={len(val_loader)}')\n",
    "    \n",
    "    if cfg['model_type'] == 'cnn_lstm':\n",
    "        emb = torch.tensor(glove, dtype=torch.float32) if cfg.get('use_glove') and glove is not None else None\n",
    "        model = create_cnn_model(embedding_matrix=emb, vocab_size=vocab_size, embed_dim=cfg['embed_dim'],\n",
    "            decoder_dim=cfg.get('hidden_dim',512), attention_dim=cfg.get('attention_dim',256), dropout=cfg['dropout'])\n",
    "    else:\n",
    "        model = VisionTransformerCaptioning(vocab_size=vocab_size, embed_dim=cfg['embed_dim'],\n",
    "            num_heads=cfg['num_heads'], num_encoder_layers=cfg['num_layers'], num_decoder_layers=cfg['num_layers'],\n",
    "            ff_dim=cfg['ff_dim'], max_seq_len=30, dropout=cfg['dropout'], img_size=128, patch_size=16)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    print(f'Params: {sum(p.numel() for p in model.parameters()):,}')\n",
    "    \n",
    "    trainer = Trainer(model=model, train_loader=train_loader, val_loader=val_loader, text_preprocessor=text_proc,\n",
    "        learning_rate=cfg['learning_rate'], device=DEVICE, checkpoint_dir=ckpt_dir, output_dir=out_dir)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    history = trainer.train(num_epochs=cfg['epochs'])\n",
    "    dur = (time.time()-t0)/60\n",
    "    \n",
    "    res = {'name': name, 'duration_min': dur, 'best_bleu': max(history['val_bleu']), 'best_loss': min(history['val_loss'])}\n",
    "    with open(f'{out_dir}/results.json','w') as f: json.dump(res, f, indent=2)\n",
    "    print(f'Done: {dur:.1f}min, BLEU={res[\"best_bleu\"]:.4f}')\n",
    "    return res, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f057c4",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "- Option A: Run all 3 (8-10 hrs)\n",
    "- Option B: Run individual cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION A: ALL MODELS\n",
    "for name, cfg in CONFIGS.items():\n",
    "    try: train_model(name, cfg)\n",
    "    except Exception as e: print(f'ERROR {name}: {e}')\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240def9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B-1: CNN Large only\n",
    "train_model('colab_cnn_large', CONFIGS['colab_cnn_large'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B-2: ViT only\n",
    "train_model('colab_vit_standard', CONFIGS['colab_vit_standard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ed8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B-3: CNN+GloVe only\n",
    "train_model('colab_cnn_glove', CONFIGS['colab_cnn_glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Drive\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "for n in CONFIGS:\n",
    "    !cp -r \"checkpoints/{n}\" \"{DRIVE_PATH}/checkpoints/\" 2>/dev/null || true\n",
    "    !cp -r \"outputs/{n}\" \"{DRIVE_PATH}/outputs/\" 2>/dev/null || true\n",
    "print('Saved to Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available(): print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd7292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nltk gensim pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59480d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "for f in ['data/processed/vocabulary.json', 'train.py', 'models/cnn_lstm.py']:\n",
    "    print('âœ“' if os.path.exists(f'{DRIVE_PATH}/{f}') else 'âœ—', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/artemis\n",
    "!cp -r \"{DRIVE_PATH}/data\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/utils\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/models\" /content/artemis/\n",
    "!cp \"{DRIVE_PATH}/train.py\" /content/artemis/\n",
    "print('âœ“ Copied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a307b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "os.chdir('/content/artemis')\n",
    "sys.path.insert(0, '/content/artemis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, time, numpy as np\n",
    "from utils.text_preprocessing import TextPreprocessor\n",
    "from utils.data_loader import create_data_loaders\n",
    "from models.cnn_lstm import create_model as create_cnn_model\n",
    "from models.vision_transformer import VisionTransformerCaptioning\n",
    "from train import Trainer\n",
    "print('âœ“ Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "CONFIGS = {\n",
    "    'colab_cnn_large': {\n",
    "        'model_type': 'cnn_lstm', 'batch_size': 32, 'num_images': 15000,\n",
    "        'epochs': 50, 'learning_rate': 1e-4, 'embed_dim': 512,\n",
    "        'hidden_dim': 1024, 'attention_dim': 512, 'dropout': 0.4,\n",
    "    },\n",
    "    'colab_vit_standard': {\n",
    "        'model_type': 'vit', 'batch_size': 32, 'num_images': 15000,\n",
    "        'epochs': 50, 'learning_rate': 1e-4, 'embed_dim': 256,\n",
    "        'num_heads': 8, 'num_layers': 6, 'ff_dim': 1024, 'dropout': 0.1,\n",
    "    },\n",
    "    'colab_cnn_glove': {\n",
    "        'model_type': 'cnn_lstm', 'batch_size': 32, 'num_images': 15000,\n",
    "        'epochs': 40, 'learning_rate': 5e-5, 'embed_dim': 300,\n",
    "        'hidden_dim': 512, 'attention_dim': 256, 'dropout': 0.3, 'use_glove': True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6381254",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_proc = TextPreprocessor()\n",
    "text_proc.load_vocabulary('data/processed/vocabulary.json')\n",
    "vocab_size = text_proc.vocab_size\n",
    "print(f'Vocab: {vocab_size}')\n",
    "\n",
    "glove = None\n",
    "if os.path.exists('data/embeddings/glove_embeddings.npy'):\n",
    "    glove = np.load('data/embeddings/glove_embeddings.npy')\n",
    "    print(f'GloVe: {glove.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75095494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitedLoader:\n",
    "    def __init__(self, loader, max_batches):\n",
    "        self.loader = loader\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_size = loader.batch_size\n",
    "        self.dataset = loader.dataset\n",
    "    def __iter__(self):\n",
    "        for i, b in enumerate(self.loader):\n",
    "            if i >= self.max_batches: break\n",
    "            yield b\n",
    "    def __len__(self): return min(len(self.loader), self.max_batches)\n",
    "\n",
    "def train_model(name, cfg):\n",
    "    print(f'\\n{\"=\"*70}\\nTRAINING: {name}\\n{\"=\"*70}')\n",
    "    ckpt_dir, out_dir = f'checkpoints/{name}', f'outputs/{name}'\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    loaders = create_data_loaders(text_preprocessor=text_proc, batch_size=cfg['batch_size'], num_workers=NUM_WORKERS, splits=['train','val'])\n",
    "    train_loader = LimitedLoader(loaders['train'], cfg['num_images']//cfg['batch_size'])\n",
    "    val_loader = LimitedLoader(loaders['val'], 50)\n",
    "    print(f'Batches: train={len(train_loader)}, val={len(val_loader)}')\n",
    "    \n",
    "    if cfg['model_type'] == 'cnn_lstm':\n",
    "        emb = torch.tensor(glove, dtype=torch.float32) if cfg.get('use_glove') and glove is not None else None\n",
    "        model = create_cnn_model(embedding_matrix=emb, vocab_size=vocab_size, embed_dim=cfg['embed_dim'],\n",
    "            decoder_dim=cfg.get('hidden_dim',512), attention_dim=cfg.get('attention_dim',256), dropout=cfg['dropout'])\n",
    "    else:\n",
    "        model = VisionTransformerCaptioning(vocab_size=vocab_size, embed_dim=cfg['embed_dim'],\n",
    "            num_heads=cfg['num_heads'], num_encoder_layers=cfg['num_layers'], num_decoder_layers=cfg['num_layers'],\n",
    "            ff_dim=cfg['ff_dim'], max_seq_len=30, dropout=cfg['dropout'], img_size=128, patch_size=16)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    print(f'Params: {sum(p.numel() for p in model.parameters()):,}')\n",
    "    \n",
    "    trainer = Trainer(model=model, train_loader=train_loader, val_loader=val_loader, text_preprocessor=text_proc,\n",
    "        learning_rate=cfg['learning_rate'], device=DEVICE, checkpoint_dir=ckpt_dir, output_dir=out_dir)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    history = trainer.train(num_epochs=cfg['epochs'])\n",
    "    dur = (time.time()-t0)/60\n",
    "    \n",
    "    res = {'name': name, 'duration_min': dur, 'best_bleu': max(history['val_bleu']), 'best_loss': min(history['val_loss'])}\n",
    "    with open(f'{out_dir}/results.json','w') as f: json.dump(res, f, indent=2)\n",
    "    print(f'Done: {dur:.1f}min, BLEU={res[\"best_bleu\"]:.4f}')\n",
    "    return res, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36387ec",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "- **Option A:** Run all 3 (~8-10 hrs)\n",
    "- **Option B:** Run individual cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION A: ALL MODELS\n",
    "for name, cfg in CONFIGS.items():\n",
    "    try: train_model(name, cfg)\n",
    "    except Exception as e: print(f'ERROR {name}: {e}')\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba021a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B-1: CNN Large only\n",
    "train_model('colab_cnn_large', CONFIGS['colab_cnn_large'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B-2: ViT only\n",
    "train_model('colab_vit_standard', CONFIGS['colab_vit_standard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B-3: CNN+GloVe only\n",
    "train_model('colab_cnn_glove', CONFIGS['colab_cnn_glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Drive\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "for n in CONFIGS:\n",
    "    if os.path.exists(f'checkpoints/{n}'): !cp -r \"checkpoints/{n}\" \"{DRIVE_PATH}/checkpoints/\"\n",
    "    if os.path.exists(f'outputs/{n}'): !cp -r \"outputs/{n}\" \"{DRIVE_PATH}/outputs/\"\n",
    "print('âœ“ Saved to Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aca1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    raise RuntimeError(\"No GPU! Enable in Runtime -> Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e15bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install packages\n",
    "!pip install -q nltk gensim pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61beb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f355b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Set paths and verify files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "\n",
    "required = [\n",
    "    'data/processed/vocabulary.json',\n",
    "    'data/processed/splits/train.json',\n",
    "    'train.py',\n",
    "    'models/cnn_lstm.py',\n",
    "    'utils/data_loader.py',\n",
    "    'utils/text_preprocessing.py'\n",
    "]\n",
    "\n",
    "print(\"Checking files...\")\n",
    "for f in required:\n",
    "    path = os.path.join(DRIVE_PATH, f)\n",
    "    status = \"âœ“\" if os.path.exists(path) else \"âœ— MISSING\"\n",
    "    print(f\"  {status} {f}\")\n",
    "\n",
    "# Count images\n",
    "img_path = os.path.join(DRIVE_PATH, 'data/processed/images')\n",
    "if os.path.exists(img_path):\n",
    "    n = sum(1 for d in os.listdir(img_path) if os.path.isdir(os.path.join(img_path, d))\n",
    "            for _ in os.listdir(os.path.join(img_path, d)) if _.endswith('.jpg'))\n",
    "    print(f\"  âœ“ Images: ~{n} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d282340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Copy to local for speed\n",
    "!mkdir -p /content/artemis\n",
    "!cp -r \"{DRIVE_PATH}/data\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/utils\" /content/artemis/\n",
    "!cp -r \"{DRIVE_PATH}/models\" /content/artemis/\n",
    "!cp \"{DRIVE_PATH}/train.py\" /content/artemis/\n",
    "print(\"âœ“ Copied to /content/artemis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Setup Python path\n",
    "import sys\n",
    "os.chdir('/content/artemis')\n",
    "sys.path.insert(0, '/content/artemis')\n",
    "print(f\"Working dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67838f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Imports\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.text_preprocessing import TextPreprocessor\n",
    "from utils.data_loader import create_data_loaders\n",
    "from models.cnn_lstm import create_model as create_cnn_model\n",
    "from models.vision_transformer import VisionTransformerCaptioning\n",
    "from train import Trainer\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "CONFIGS = {\n",
    "    'colab_cnn_large': {\n",
    "        'model_type': 'cnn_lstm',\n",
    "        'description': 'CNN+LSTM Large (512 embed, 1024 hidden)',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 1e-4,\n",
    "        'embed_dim': 512,\n",
    "        'hidden_dim': 1024,\n",
    "        'attention_dim': 512,\n",
    "        'dropout': 0.4,\n",
    "    },\n",
    "    'colab_vit_standard': {\n",
    "        'model_type': 'vit',\n",
    "        'description': 'Vision Transformer (6 layers)',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 1e-4,\n",
    "        'embed_dim': 256,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 6,\n",
    "        'ff_dim': 1024,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    'colab_cnn_glove': {\n",
    "        'model_type': 'cnn_lstm',\n",
    "        'description': 'CNN+LSTM with GloVe embeddings',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 40,\n",
    "        'learning_rate': 5e-5,\n",
    "        'embed_dim': 300,\n",
    "        'hidden_dim': 512,\n",
    "        'attention_dim': 256,\n",
    "        'dropout': 0.3,\n",
    "        'use_glove': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configurations:\")\n",
    "for name, cfg in CONFIGS.items():\n",
    "    print(f\"  {name}: {cfg['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Load vocabulary\n",
    "text_proc = TextPreprocessor()\n",
    "text_proc.load_vocabulary('data/processed/vocabulary.json')\n",
    "vocab_size = text_proc.vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Load GloVe if available\n",
    "glove_embeddings = None\n",
    "if os.path.exists('data/embeddings/glove_embeddings.npy'):\n",
    "    glove_embeddings = np.load('data/embeddings/glove_embeddings.npy')\n",
    "    print(f\"âœ“ GloVe: {glove_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf175fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Helper classes and functions\n",
    "\n",
    "class LimitedLoader:\n",
    "    \"\"\"Wrapper to limit batches per epoch.\"\"\"\n",
    "    def __init__(self, loader, max_batches):\n",
    "        self.loader = loader\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_size = loader.batch_size\n",
    "        self.dataset = loader.dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, batch in enumerate(self.loader):\n",
    "            if i >= self.max_batches:\n",
    "                break\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.loader), self.max_batches)\n",
    "\n",
    "\n",
    "def train_model(config_name, config):\n",
    "    \"\"\"Train a single model.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"TRAINING: {config_name}\")\n",
    "    print(f\"{config['description']}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    checkpoint_dir = f'checkpoints/{config_name}'\n",
    "    output_dir = f'outputs/{config_name}'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save config\n",
    "    with open(f'{output_dir}/config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    # Data loaders (using correct API)\n",
    "    print(\"\\nCreating data loaders...\")\n",
    "    loaders = create_data_loaders(\n",
    "        text_preprocessor=text_proc,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=NUM_WORKERS,\n",
    "        splits=['train', 'val']\n",
    "    )\n",
    "\n",
    "    max_train = config['num_images'] // config['batch_size']\n",
    "    max_val = max(30, max_train // 5)\n",
    "\n",
    "    train_loader = LimitedLoader(loaders['train'], max_train)\n",
    "    val_loader = LimitedLoader(loaders['val'], max_val)\n",
    "    print(f\"  Train: {len(train_loader)} batches, Val: {len(val_loader)} batches\")\n",
    "\n",
    "    # Create model\n",
    "    print(\"\\nCreating model...\")\n",
    "    if config['model_type'] == 'cnn_lstm':\n",
    "        embeddings = None\n",
    "        if config.get('use_glove') and glove_embeddings is not None:\n",
    "            embeddings = torch.tensor(glove_embeddings, dtype=torch.float32)\n",
    "            print(\"  Using GloVe embeddings\")\n",
    "\n",
    "        model = create_cnn_model(\n",
    "            embedding_matrix=embeddings,\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=config['embed_dim'],\n",
    "            decoder_dim=config.get('hidden_dim', 512),\n",
    "            attention_dim=config.get('attention_dim', 256),\n",
    "            dropout=config['dropout'],\n",
    "            pretrained_cnn=True\n",
    "        )\n",
    "    else:\n",
    "        model = VisionTransformerCaptioning(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=config['embed_dim'],\n",
    "            num_heads=config['num_heads'],\n",
    "            num_encoder_layers=config['num_layers'],\n",
    "            num_decoder_layers=config['num_layers'],\n",
    "            ff_dim=config['ff_dim'],\n",
    "            max_seq_len=30,\n",
    "            dropout=config['dropout'],\n",
    "            img_size=128,\n",
    "            patch_size=16\n",
    "        )\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Parameters: {params:,}\")\n",
    "\n",
    "    # Trainer (learning_rate in __init__)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        text_preprocessor=text_proc,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        device=DEVICE,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    # Train (num_epochs in train())\n",
    "    print(f\"\\nTraining for {config['epochs']} epochs...\")\n",
    "    start = time.time()\n",
    "    history = trainer.train(num_epochs=config['epochs'])\n",
    "    duration = time.time() - start\n",
    "\n",
    "    # Results\n",
    "    results = {\n",
    "        'config_name': config_name,\n",
    "        'description': config['description'],\n",
    "        'parameters': params,\n",
    "        'epochs': config['epochs'],\n",
    "        'duration_minutes': duration / 60,\n",
    "        'final_train_loss': history['train_loss'][-1],\n",
    "        'final_val_loss': history['val_loss'][-1],\n",
    "        'best_val_loss': min(history['val_loss']),\n",
    "        'best_bleu': max(history['val_bleu']),\n",
    "    }\n",
    "\n",
    "    with open(f'{output_dir}/results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"DONE: {config_name}\")\n",
    "    print(f\"Duration: {duration/60:.1f} min, Best BLEU: {results['best_bleu']:.4f}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    return results, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72c626",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "**Option A:** Run all 3 sequentially (~8-10 hours)\n",
    "\n",
    "**Option B:** Run individual cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a369c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION A: Train ALL models\n",
    "all_results = {}\n",
    "all_history = {}\n",
    "\n",
    "for name, cfg in CONFIGS.items():\n",
    "    try:\n",
    "        results, history = train_model(name, cfg)\n",
    "        all_results[name] = results\n",
    "        all_history[name] = history\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {name} failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Clear GPU\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c306961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B-1: Train CNN+LSTM Large only (~3-4 hrs)\n",
    "name = 'colab_cnn_large'\n",
    "results_1, history_1 = train_model(name, CONFIGS[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B-2: Train ViT only (~2-3 hrs)\n",
    "name = 'colab_vit_standard'\n",
    "results_2, history_2 = train_model(name, CONFIGS[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aeaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B-3: Train CNN+GloVe only (~2-3 hrs)\n",
    "name = 'colab_cnn_glove'\n",
    "results_3, history_3 = train_model(name, CONFIGS[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3b888",
   "metadata": {},
   "source": [
    "## Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ab34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy results back to Drive\n",
    "DRIVE_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "\n",
    "!mkdir -p \"{DRIVE_PATH}/checkpoints\"\n",
    "!mkdir -p \"{DRIVE_PATH}/outputs\"\n",
    "\n",
    "for name in CONFIGS.keys():\n",
    "    if os.path.exists(f'checkpoints/{name}'):\n",
    "        !cp -r \"checkpoints/{name}\" \"{DRIVE_PATH}/checkpoints/\"\n",
    "        print(f\"âœ“ Saved checkpoints/{name}\")\n",
    "    if os.path.exists(f'outputs/{name}'):\n",
    "        !cp -r \"outputs/{name}\" \"{DRIVE_PATH}/outputs/\"\n",
    "        print(f\"âœ“ Saved outputs/{name}\")\n",
    "\n",
    "print(f\"\\nâœ“ All saved to {DRIVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb7ff4",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cef549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"No GPU available! Please enable GPU in Runtime -> Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q nltk gensim pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c2fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b312362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths - MODIFY THIS to match your Drive folder\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_DATA_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "\n",
    "# Check if data exists\n",
    "required_files = [\n",
    "    'data/processed/vocabulary.json',\n",
    "    'data/processed/splits/train.json',\n",
    "    'train.py',\n",
    "    'models/cnn_lstm.py',\n",
    "    'utils/data_loader.py'\n",
    "]\n",
    "\n",
    "print(\"Checking required files...\")\n",
    "all_found = True\n",
    "for f in required_files:\n",
    "    path = os.path.join(DRIVE_DATA_PATH, f)\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  âœ“ {f}\")\n",
    "    else:\n",
    "        print(f\"  âœ— {f} - NOT FOUND\")\n",
    "        all_found = False\n",
    "\n",
    "# Check for preprocessed images\n",
    "preprocessed_path = os.path.join(DRIVE_DATA_PATH, 'data/processed/images')\n",
    "if os.path.exists(preprocessed_path) and os.listdir(preprocessed_path):\n",
    "    num_imgs = sum(1 for _ in Path(preprocessed_path).rglob('*.jpg'))\n",
    "    print(f\"  âœ“ Preprocessed images: {num_imgs} files\")\n",
    "else:\n",
    "    print(f\"  âš  No preprocessed images found - will use raw wikiart\")\n",
    "\n",
    "if not all_found:\n",
    "    raise FileNotFoundError(\"Missing required files! See above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f510632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to local storage for faster access\n",
    "!mkdir -p /content/artemis\n",
    "!cp -r \"{DRIVE_DATA_PATH}/data\" /content/artemis/\n",
    "!cp -r \"{DRIVE_DATA_PATH}/utils\" /content/artemis/\n",
    "!cp -r \"{DRIVE_DATA_PATH}/models\" /content/artemis/\n",
    "!cp \"{DRIVE_DATA_PATH}/train.py\" /content/artemis/\n",
    "\n",
    "print(\"âœ“ Data copied to /content/artemis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f05ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python path\n",
    "import sys\n",
    "os.chdir('/content/artemis')\n",
    "sys.path.insert(0, '/content/artemis')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from utils.data_loader import create_dataloaders\n",
    "from utils.evaluation import BLEUScore\n",
    "from train import Trainer\n",
    "\n",
    "print(\"âœ“ All modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b484fcf",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Three model configurations for Colab\n",
    "COLAB_CONFIGS = {\n",
    "    'colab_cnn_large': {\n",
    "        'model_type': 'cnn_lstm',\n",
    "        'description': 'CNN+LSTM Large Capacity',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 1e-4,\n",
    "        'embed_dim': 512,\n",
    "        'hidden_dim': 1024,\n",
    "        'attention_dim': 512,\n",
    "        'dropout': 0.4,\n",
    "        'encoder_lr_factor': 0.1,\n",
    "    },\n",
    "    'colab_vit_standard': {\n",
    "        'model_type': 'vit',\n",
    "        'description': 'Vision Transformer Standard',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 1e-4,\n",
    "        'embed_dim': 256,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 6,\n",
    "        'ff_dim': 1024,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    'colab_cnn_glove': {\n",
    "        'model_type': 'cnn_lstm',\n",
    "        'description': 'CNN+LSTM with GloVe Embeddings',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 40,  # Slightly fewer since pretrained embeddings converge faster\n",
    "        'learning_rate': 5e-5,  # Lower LR for pretrained embeddings\n",
    "        'embed_dim': 300,  # GloVe dimension\n",
    "        'hidden_dim': 512,\n",
    "        'attention_dim': 256,\n",
    "        'dropout': 0.3,\n",
    "        'encoder_lr_factor': 0.1,\n",
    "        'use_glove': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Training Configurations:\")\n",
    "print(\"=\" * 70)\n",
    "for name, cfg in COLAB_CONFIGS.items():\n",
    "    print(f\"\\n{name}: {cfg['description']}\")\n",
    "    print(f\"  Model: {cfg['model_type']}\")\n",
    "    print(f\"  Images: ~{cfg['num_images']}, Epochs: {cfg['epochs']}\")\n",
    "    print(f\"  Batch size: {cfg['batch_size']}, LR: {cfg['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "with open('data/processed/vocabulary.json', 'r', encoding='utf-8') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "vocab_size = vocab_data['vocab_size']\n",
    "word_to_idx = vocab_data['word2idx']\n",
    "idx_to_word = {int(k): v for k, v in vocab_data['idx2word'].items()}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings if available\n",
    "glove_embeddings = None\n",
    "glove_path = 'data/embeddings/glove_embeddings.npy'\n",
    "if os.path.exists(glove_path):\n",
    "    glove_embeddings = np.load(glove_path)\n",
    "    print(f\"âœ“ GloVe embeddings loaded: {glove_embeddings.shape}\")\n",
    "else:\n",
    "    print(\"âš  GloVe embeddings not found - will use random initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b7905",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ad6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(config):\n",
    "    \"\"\"Create CNN+LSTM model.\"\"\"\n",
    "    from models.cnn_lstm import ImageCaptioningModel\n",
    "    \n",
    "    model = ImageCaptioningModel(\n",
    "        embed_dim=config['embed_dim'],\n",
    "        attention_dim=config['attention_dim'],\n",
    "        decoder_dim=config['hidden_dim'],\n",
    "        vocab_size=vocab_size,\n",
    "        encoder_dim=2048,\n",
    "        dropout=config['dropout'],\n",
    "        pretrained_encoder=True\n",
    "    )\n",
    "    \n",
    "    # Load GloVe embeddings if specified\n",
    "    if config.get('use_glove') and glove_embeddings is not None:\n",
    "        model.decoder.embedding.weight.data.copy_(\n",
    "            torch.tensor(glove_embeddings, dtype=torch.float32)\n",
    "        )\n",
    "        print(\"  âœ“ GloVe embeddings loaded into model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_vit_model(config):\n",
    "    \"\"\"Create Vision Transformer model.\"\"\"\n",
    "    from models.vision_transformer import VisionTransformerCaptioning\n",
    "    \n",
    "    model = VisionTransformerCaptioning(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=config['embed_dim'],\n",
    "        num_heads=config['num_heads'],\n",
    "        num_encoder_layers=config['num_layers'],\n",
    "        num_decoder_layers=config['num_layers'],\n",
    "        ff_dim=config['ff_dim'],\n",
    "        max_seq_len=30,\n",
    "        dropout=config['dropout'],\n",
    "        img_size=128,\n",
    "        patch_size=16\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "class LimitedLoader:\n",
    "    \"\"\"Wrapper to limit batches per epoch.\"\"\"\n",
    "    def __init__(self, loader, max_batches):\n",
    "        self.loader = loader\n",
    "        self.max_batches = max_batches\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i, batch in enumerate(self.loader):\n",
    "            if i >= self.max_batches:\n",
    "                break\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(len(self.loader), self.max_batches)\n",
    "\n",
    "\n",
    "def train_model(config_name, config):\n",
    "    \"\"\"Train a single model configuration.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"TRAINING: {config_name}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create directories\n",
    "    checkpoint_dir = f'checkpoints/{config_name}'\n",
    "    output_dir = f'outputs/{config_name}'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save config\n",
    "    with open(f'{output_dir}/config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Create data loaders\n",
    "    print(\"\\nCreating data loaders...\")\n",
    "    train_loader, val_loader, _ = create_dataloaders(\n",
    "        images_dir='data/processed/images',\n",
    "        captions_dir='data/processed/captions',\n",
    "        splits_dir='data/processed/splits',\n",
    "        vocab_file='data/processed/vocabulary.json',\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    # Limit batches\n",
    "    max_train_batches = config['num_images'] // config['batch_size']\n",
    "    max_val_batches = max(20, max_train_batches // 5)\n",
    "    \n",
    "    train_loader = LimitedLoader(train_loader, max_train_batches)\n",
    "    val_loader = LimitedLoader(val_loader, max_val_batches)\n",
    "    \n",
    "    print(f\"  Train batches: {len(train_loader)} (~{len(train_loader) * config['batch_size']} images)\")\n",
    "    print(f\"  Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating model...\")\n",
    "    if config['model_type'] == 'cnn_lstm':\n",
    "        model = create_cnn_lstm_model(config)\n",
    "    else:\n",
    "        model = create_vit_model(config)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Parameters: {total_params:,}\")\n",
    "    \n",
    "    # Create optimizer\n",
    "    if config['model_type'] == 'cnn_lstm':\n",
    "        encoder_params = list(model.encoder.parameters())\n",
    "        decoder_params = list(model.decoder.parameters()) + list(model.attention.parameters())\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': encoder_params, 'lr': config['learning_rate'] * config['encoder_lr_factor']},\n",
    "            {'params': decoder_params, 'lr': config['learning_rate']}\n",
    "        ], weight_decay=1e-5)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    bleu_scorer = BLEUScore(idx_to_word, word_to_idx)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        evaluator=bleu_scorer,\n",
    "        device=DEVICE,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        grad_clip=5.0\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nStarting training for {config['epochs']} epochs...\")\n",
    "    start_time = time.time()\n",
    "    history = trainer.train(num_epochs=config['epochs'])\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'config_name': config_name,\n",
    "        'description': config['description'],\n",
    "        'model_type': config['model_type'],\n",
    "        'num_images': config['num_images'],\n",
    "        'epochs': config['epochs'],\n",
    "        'parameters': total_params,\n",
    "        'final_train_loss': history[-1]['train_loss'],\n",
    "        'final_val_loss': history[-1]['val_loss'],\n",
    "        'best_val_loss': min(h['val_loss'] for h in history),\n",
    "        'best_bleu': max(h.get('bleu', 0) for h in history),\n",
    "        'duration_minutes': duration / 60,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    with open(f'{output_dir}/results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"COMPLETED: {config_name}\")\n",
    "    print(f\"Duration: {duration/60:.1f} minutes\")\n",
    "    print(f\"Best Val Loss: {results['best_val_loss']:.4f}\")\n",
    "    print(f\"Best BLEU: {results['best_bleu']:.4f}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c2e20",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "**Option A:** Run next cell to train ALL 3 models sequentially (~8-10 hours total)\n",
    "\n",
    "**Option B:** Use the individual training cells below to train one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa91b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all three configurations\n",
    "all_results = {}\n",
    "\n",
    "for config_name, config in COLAB_CONFIGS.items():\n",
    "    try:\n",
    "        results = train_model(config_name, config)\n",
    "        all_results[config_name] = results\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error training {config_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        all_results[config_name] = {'error': str(e)}\n",
    "    \n",
    "    # Clear GPU memory between models\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e6e8c8",
   "metadata": {},
   "source": [
    "### Option B: Train Individual Models (run only ONE of these cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479099f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 1: CNN+LSTM Large (~3-4 hours)\n",
    "config_name = 'colab_cnn_large'\n",
    "results_1 = train_model(config_name, COLAB_CONFIGS[config_name])\n",
    "print(f\"\\nâœ… Model 1 complete! Best BLEU: {results_1['best_bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae581931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 2: ViT Standard (~2-3 hours)\n",
    "config_name = 'colab_vit_standard'\n",
    "results_2 = train_model(config_name, COLAB_CONFIGS[config_name])\n",
    "print(f\"\\nâœ… Model 2 complete! Best BLEU: {results_2['best_bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07990576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 3: CNN+LSTM with GloVe (~2-3 hours)\n",
    "config_name = 'colab_cnn_glove'\n",
    "results_3 = train_model(config_name, COLAB_CONFIGS[config_name])\n",
    "print(f\"\\nâœ… Model 3 complete! Best BLEU: {results_3['best_bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f831193",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6302f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    if 'error' in results:\n",
    "        print(f\"\\n{name}: FAILED - {results['error']}\")\n",
    "    else:\n",
    "        print(f\"\\n{name}: {results['description']}\")\n",
    "        print(f\"  Parameters: {results['parameters']:,}\")\n",
    "        print(f\"  Duration: {results['duration_minutes']:.1f} minutes\")\n",
    "        print(f\"  Final Val Loss: {results['final_val_loss']:.4f}\")\n",
    "        print(f\"  Best BLEU: {results['best_bleu']:.4f}\")\n",
    "\n",
    "# Save combined results\n",
    "with open('outputs/colab_all_results.json', 'w') as f:\n",
    "    # Convert history to serializable format\n",
    "    save_results = {}\n",
    "    for name, res in all_results.items():\n",
    "        if 'error' not in res:\n",
    "            save_results[name] = {k: v for k, v in res.items() if k != 'history'}\n",
    "        else:\n",
    "            save_results[name] = res\n",
    "    json.dump(save_results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ“ Results saved to outputs/colab_all_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31bb941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['blue', 'green', 'orange']\n",
    "for i, (name, results) in enumerate(all_results.items()):\n",
    "    if 'error' in results:\n",
    "        continue\n",
    "    \n",
    "    history = results['history']\n",
    "    epochs = range(1, len(history) + 1)\n",
    "    val_losses = [h['val_loss'] for h in history]\n",
    "    bleu_scores = [h.get('bleu', 0) for h in history]\n",
    "    \n",
    "    axes[0].plot(epochs, val_losses, color=colors[i], label=name)\n",
    "    axes[1].plot(epochs, bleu_scores, color=colors[i], label=name)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].set_title('Validation Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('BLEU Score')\n",
    "axes[1].set_title('BLEU Score Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/colab_training_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07266e4",
   "metadata": {},
   "source": [
    "## 6. Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f283818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy results to Google Drive\n",
    "!mkdir -p \"{DRIVE_DATA_PATH}/checkpoints\"\n",
    "!mkdir -p \"{DRIVE_DATA_PATH}/outputs\"\n",
    "\n",
    "# Copy all checkpoint and output folders\n",
    "for name in COLAB_CONFIGS.keys():\n",
    "    if os.path.exists(f'checkpoints/{name}'):\n",
    "        !cp -r \"checkpoints/{name}\" \"{DRIVE_DATA_PATH}/checkpoints/\"\n",
    "    if os.path.exists(f'outputs/{name}'):\n",
    "        !cp -r \"outputs/{name}\" \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "\n",
    "# Copy combined results\n",
    "!cp outputs/colab_all_results.json \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "!cp outputs/colab_training_comparison.png \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "\n",
    "print(\"\\nâœ“ All results saved to Google Drive!\")\n",
    "print(f\"Location: {DRIVE_DATA_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
