{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56dcd467",
   "metadata": {},
   "source": [
    "# ArtEmis Image Captioning - Multi-Model Training (Colab T4 GPU)\n",
    "\n",
    "## üìã SETUP CHECKLIST (Complete Before Running)\n",
    "\n",
    "**Before running this notebook:**\n",
    "\n",
    "1. **Enable GPU Runtime:**\n",
    "   - Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `T4 GPU`\n",
    "\n",
    "2. **Upload Data to Google Drive:**\n",
    "   - Create folder: `Google Drive/artemis-captioning/`\n",
    "   - Upload these from your local machine:\n",
    "     ```\n",
    "     artemis-captioning/\n",
    "     ‚îú‚îÄ‚îÄ data/\n",
    "     ‚îÇ   ‚îú‚îÄ‚îÄ processed/\n",
    "     ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ images/        (5000 pre-resized 128x128 images, ~57 MB)\n",
    "     ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ splits/        (train.json, val.json, test.json)\n",
    "     ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ captions/      (caption JSON files)\n",
    "     ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vocabulary.json\n",
    "     ‚îÇ   ‚îî‚îÄ‚îÄ embeddings/        (optional - glove, word2vec, tfidf)\n",
    "     ‚îú‚îÄ‚îÄ utils/                 (all Python files)\n",
    "     ‚îú‚îÄ‚îÄ models/                (all Python files)\n",
    "     ‚îî‚îÄ‚îÄ train.py\n",
    "     ```\n",
    "\n",
    "3. **Update DRIVE_DATA_PATH** in Cell 6 if needed (default: `/content/drive/MyDrive/artemis-captioning`)\n",
    "\n",
    "## üöÄ Training Configurations\n",
    "\n",
    "| Config | Model | Images | Epochs | Est. Time |\n",
    "|--------|-------|--------|--------|-----------|\n",
    "| colab_cnn_large | CNN+LSTM (512 embed, 1024 hidden) | 15,000 | 50 | ~3-4 hours |\n",
    "| colab_vit_standard | ViT (256 embed, 6 layers) | 15,000 | 50 | ~2-3 hours |\n",
    "| colab_cnn_glove | CNN+LSTM + GloVe embeddings | 15,000 | 40 | ~2-3 hours |\n",
    "\n",
    "**Total estimated time: ~8-10 hours** (run overnight or train one at a time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb7ff4",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cef549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"No GPU available! Please enable GPU in Runtime -> Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q nltk gensim pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c2fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b312362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths - MODIFY THIS to match your Drive folder\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_DATA_PATH = '/content/drive/MyDrive/artemis-captioning'\n",
    "\n",
    "# Check if data exists\n",
    "required_files = [\n",
    "    'data/processed/vocabulary.json',\n",
    "    'data/processed/splits/train.json',\n",
    "    'train.py',\n",
    "    'models/cnn_lstm.py',\n",
    "    'utils/data_loader.py'\n",
    "]\n",
    "\n",
    "print(\"Checking required files...\")\n",
    "all_found = True\n",
    "for f in required_files:\n",
    "    path = os.path.join(DRIVE_DATA_PATH, f)\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  ‚úì {f}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {f} - NOT FOUND\")\n",
    "        all_found = False\n",
    "\n",
    "# Check for preprocessed images\n",
    "preprocessed_path = os.path.join(DRIVE_DATA_PATH, 'data/processed/images')\n",
    "if os.path.exists(preprocessed_path) and os.listdir(preprocessed_path):\n",
    "    num_imgs = sum(1 for _ in Path(preprocessed_path).rglob('*.jpg'))\n",
    "    print(f\"  ‚úì Preprocessed images: {num_imgs} files\")\n",
    "else:\n",
    "    print(f\"  ‚ö† No preprocessed images found - will use raw wikiart\")\n",
    "\n",
    "if not all_found:\n",
    "    raise FileNotFoundError(\"Missing required files! See above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f510632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to local storage for faster access\n",
    "!mkdir -p /content/artemis\n",
    "!cp -r \"{DRIVE_DATA_PATH}/data\" /content/artemis/\n",
    "!cp -r \"{DRIVE_DATA_PATH}/utils\" /content/artemis/\n",
    "!cp -r \"{DRIVE_DATA_PATH}/models\" /content/artemis/\n",
    "!cp \"{DRIVE_DATA_PATH}/train.py\" /content/artemis/\n",
    "\n",
    "print(\"‚úì Data copied to /content/artemis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f05ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python path\n",
    "import sys\n",
    "os.chdir('/content/artemis')\n",
    "sys.path.insert(0, '/content/artemis')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from utils.data_loader import create_dataloaders\n",
    "from utils.evaluation import BLEUScore\n",
    "from train import Trainer\n",
    "\n",
    "print(\"‚úì All modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b484fcf",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Three model configurations for Colab\n",
    "COLAB_CONFIGS = {\n",
    "    'colab_cnn_large': {\n",
    "        'model_type': 'cnn_lstm',\n",
    "        'description': 'CNN+LSTM Large Capacity',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 1e-4,\n",
    "        'embed_dim': 512,\n",
    "        'hidden_dim': 1024,\n",
    "        'attention_dim': 512,\n",
    "        'dropout': 0.4,\n",
    "        'encoder_lr_factor': 0.1,\n",
    "    },\n",
    "    'colab_vit_standard': {\n",
    "        'model_type': 'vit',\n",
    "        'description': 'Vision Transformer Standard',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 1e-4,\n",
    "        'embed_dim': 256,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 6,\n",
    "        'ff_dim': 1024,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    'colab_cnn_glove': {\n",
    "        'model_type': 'cnn_lstm',\n",
    "        'description': 'CNN+LSTM with GloVe Embeddings',\n",
    "        'batch_size': 32,\n",
    "        'num_images': 15000,\n",
    "        'epochs': 40,  # Slightly fewer since pretrained embeddings converge faster\n",
    "        'learning_rate': 5e-5,  # Lower LR for pretrained embeddings\n",
    "        'embed_dim': 300,  # GloVe dimension\n",
    "        'hidden_dim': 512,\n",
    "        'attention_dim': 256,\n",
    "        'dropout': 0.3,\n",
    "        'encoder_lr_factor': 0.1,\n",
    "        'use_glove': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Training Configurations:\")\n",
    "print(\"=\" * 70)\n",
    "for name, cfg in COLAB_CONFIGS.items():\n",
    "    print(f\"\\n{name}: {cfg['description']}\")\n",
    "    print(f\"  Model: {cfg['model_type']}\")\n",
    "    print(f\"  Images: ~{cfg['num_images']}, Epochs: {cfg['epochs']}\")\n",
    "    print(f\"  Batch size: {cfg['batch_size']}, LR: {cfg['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "with open('data/processed/vocabulary.json', 'r', encoding='utf-8') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "vocab_size = vocab_data['vocab_size']\n",
    "word_to_idx = vocab_data['word2idx']\n",
    "idx_to_word = {int(k): v for k, v in vocab_data['idx2word'].items()}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings if available\n",
    "glove_embeddings = None\n",
    "glove_path = 'data/embeddings/glove_embeddings.npy'\n",
    "if os.path.exists(glove_path):\n",
    "    glove_embeddings = np.load(glove_path)\n",
    "    print(f\"‚úì GloVe embeddings loaded: {glove_embeddings.shape}\")\n",
    "else:\n",
    "    print(\"‚ö† GloVe embeddings not found - will use random initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b7905",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ad6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(config):\n",
    "    \"\"\"Create CNN+LSTM model.\"\"\"\n",
    "    from models.cnn_lstm import ImageCaptioningModel\n",
    "    \n",
    "    model = ImageCaptioningModel(\n",
    "        embed_dim=config['embed_dim'],\n",
    "        attention_dim=config['attention_dim'],\n",
    "        decoder_dim=config['hidden_dim'],\n",
    "        vocab_size=vocab_size,\n",
    "        encoder_dim=2048,\n",
    "        dropout=config['dropout'],\n",
    "        pretrained_encoder=True\n",
    "    )\n",
    "    \n",
    "    # Load GloVe embeddings if specified\n",
    "    if config.get('use_glove') and glove_embeddings is not None:\n",
    "        model.decoder.embedding.weight.data.copy_(\n",
    "            torch.tensor(glove_embeddings, dtype=torch.float32)\n",
    "        )\n",
    "        print(\"  ‚úì GloVe embeddings loaded into model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_vit_model(config):\n",
    "    \"\"\"Create Vision Transformer model.\"\"\"\n",
    "    from models.vision_transformer import VisionTransformerCaptioning\n",
    "    \n",
    "    model = VisionTransformerCaptioning(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=config['embed_dim'],\n",
    "        num_heads=config['num_heads'],\n",
    "        num_encoder_layers=config['num_layers'],\n",
    "        num_decoder_layers=config['num_layers'],\n",
    "        ff_dim=config['ff_dim'],\n",
    "        max_seq_len=30,\n",
    "        dropout=config['dropout'],\n",
    "        img_size=128,\n",
    "        patch_size=16\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "class LimitedLoader:\n",
    "    \"\"\"Wrapper to limit batches per epoch.\"\"\"\n",
    "    def __init__(self, loader, max_batches):\n",
    "        self.loader = loader\n",
    "        self.max_batches = max_batches\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i, batch in enumerate(self.loader):\n",
    "            if i >= self.max_batches:\n",
    "                break\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(len(self.loader), self.max_batches)\n",
    "\n",
    "\n",
    "def train_model(config_name, config):\n",
    "    \"\"\"Train a single model configuration.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"TRAINING: {config_name}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create directories\n",
    "    checkpoint_dir = f'checkpoints/{config_name}'\n",
    "    output_dir = f'outputs/{config_name}'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save config\n",
    "    with open(f'{output_dir}/config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Create data loaders\n",
    "    print(\"\\nCreating data loaders...\")\n",
    "    train_loader, val_loader, _ = create_dataloaders(\n",
    "        images_dir='data/processed/images',\n",
    "        captions_dir='data/processed/captions',\n",
    "        splits_dir='data/processed/splits',\n",
    "        vocab_file='data/processed/vocabulary.json',\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    # Limit batches\n",
    "    max_train_batches = config['num_images'] // config['batch_size']\n",
    "    max_val_batches = max(20, max_train_batches // 5)\n",
    "    \n",
    "    train_loader = LimitedLoader(train_loader, max_train_batches)\n",
    "    val_loader = LimitedLoader(val_loader, max_val_batches)\n",
    "    \n",
    "    print(f\"  Train batches: {len(train_loader)} (~{len(train_loader) * config['batch_size']} images)\")\n",
    "    print(f\"  Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating model...\")\n",
    "    if config['model_type'] == 'cnn_lstm':\n",
    "        model = create_cnn_lstm_model(config)\n",
    "    else:\n",
    "        model = create_vit_model(config)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Parameters: {total_params:,}\")\n",
    "    \n",
    "    # Create optimizer\n",
    "    if config['model_type'] == 'cnn_lstm':\n",
    "        encoder_params = list(model.encoder.parameters())\n",
    "        decoder_params = list(model.decoder.parameters()) + list(model.attention.parameters())\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': encoder_params, 'lr': config['learning_rate'] * config['encoder_lr_factor']},\n",
    "            {'params': decoder_params, 'lr': config['learning_rate']}\n",
    "        ], weight_decay=1e-5)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    bleu_scorer = BLEUScore(idx_to_word, word_to_idx)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        evaluator=bleu_scorer,\n",
    "        device=DEVICE,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        grad_clip=5.0\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nStarting training for {config['epochs']} epochs...\")\n",
    "    start_time = time.time()\n",
    "    history = trainer.train(num_epochs=config['epochs'])\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'config_name': config_name,\n",
    "        'description': config['description'],\n",
    "        'model_type': config['model_type'],\n",
    "        'num_images': config['num_images'],\n",
    "        'epochs': config['epochs'],\n",
    "        'parameters': total_params,\n",
    "        'final_train_loss': history[-1]['train_loss'],\n",
    "        'final_val_loss': history[-1]['val_loss'],\n",
    "        'best_val_loss': min(h['val_loss'] for h in history),\n",
    "        'best_bleu': max(h.get('bleu', 0) for h in history),\n",
    "        'duration_minutes': duration / 60,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    with open(f'{output_dir}/results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"COMPLETED: {config_name}\")\n",
    "    print(f\"Duration: {duration/60:.1f} minutes\")\n",
    "    print(f\"Best Val Loss: {results['best_val_loss']:.4f}\")\n",
    "    print(f\"Best BLEU: {results['best_bleu']:.4f}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c2e20",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "**Option A:** Run next cell to train ALL 3 models sequentially (~8-10 hours total)\n",
    "\n",
    "**Option B:** Use the individual training cells below to train one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa91b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all three configurations\n",
    "all_results = {}\n",
    "\n",
    "for config_name, config in COLAB_CONFIGS.items():\n",
    "    try:\n",
    "        results = train_model(config_name, config)\n",
    "        all_results[config_name] = results\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error training {config_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        all_results[config_name] = {'error': str(e)}\n",
    "    \n",
    "    # Clear GPU memory between models\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e6e8c8",
   "metadata": {},
   "source": [
    "### Option B: Train Individual Models (run only ONE of these cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479099f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 1: CNN+LSTM Large (~3-4 hours)\n",
    "config_name = 'colab_cnn_large'\n",
    "results_1 = train_model(config_name, COLAB_CONFIGS[config_name])\n",
    "print(f\"\\n‚úÖ Model 1 complete! Best BLEU: {results_1['best_bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae581931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 2: ViT Standard (~2-3 hours)\n",
    "config_name = 'colab_vit_standard'\n",
    "results_2 = train_model(config_name, COLAB_CONFIGS[config_name])\n",
    "print(f\"\\n‚úÖ Model 2 complete! Best BLEU: {results_2['best_bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07990576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 3: CNN+LSTM with GloVe (~2-3 hours)\n",
    "config_name = 'colab_cnn_glove'\n",
    "results_3 = train_model(config_name, COLAB_CONFIGS[config_name])\n",
    "print(f\"\\n‚úÖ Model 3 complete! Best BLEU: {results_3['best_bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f831193",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6302f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    if 'error' in results:\n",
    "        print(f\"\\n{name}: FAILED - {results['error']}\")\n",
    "    else:\n",
    "        print(f\"\\n{name}: {results['description']}\")\n",
    "        print(f\"  Parameters: {results['parameters']:,}\")\n",
    "        print(f\"  Duration: {results['duration_minutes']:.1f} minutes\")\n",
    "        print(f\"  Final Val Loss: {results['final_val_loss']:.4f}\")\n",
    "        print(f\"  Best BLEU: {results['best_bleu']:.4f}\")\n",
    "\n",
    "# Save combined results\n",
    "with open('outputs/colab_all_results.json', 'w') as f:\n",
    "    # Convert history to serializable format\n",
    "    save_results = {}\n",
    "    for name, res in all_results.items():\n",
    "        if 'error' not in res:\n",
    "            save_results[name] = {k: v for k, v in res.items() if k != 'history'}\n",
    "        else:\n",
    "            save_results[name] = res\n",
    "    json.dump(save_results, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úì Results saved to outputs/colab_all_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31bb941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['blue', 'green', 'orange']\n",
    "for i, (name, results) in enumerate(all_results.items()):\n",
    "    if 'error' in results:\n",
    "        continue\n",
    "    \n",
    "    history = results['history']\n",
    "    epochs = range(1, len(history) + 1)\n",
    "    val_losses = [h['val_loss'] for h in history]\n",
    "    bleu_scores = [h.get('bleu', 0) for h in history]\n",
    "    \n",
    "    axes[0].plot(epochs, val_losses, color=colors[i], label=name)\n",
    "    axes[1].plot(epochs, bleu_scores, color=colors[i], label=name)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].set_title('Validation Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('BLEU Score')\n",
    "axes[1].set_title('BLEU Score Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/colab_training_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07266e4",
   "metadata": {},
   "source": [
    "## 6. Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f283818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy results to Google Drive\n",
    "!mkdir -p \"{DRIVE_DATA_PATH}/checkpoints\"\n",
    "!mkdir -p \"{DRIVE_DATA_PATH}/outputs\"\n",
    "\n",
    "# Copy all checkpoint and output folders\n",
    "for name in COLAB_CONFIGS.keys():\n",
    "    if os.path.exists(f'checkpoints/{name}'):\n",
    "        !cp -r \"checkpoints/{name}\" \"{DRIVE_DATA_PATH}/checkpoints/\"\n",
    "    if os.path.exists(f'outputs/{name}'):\n",
    "        !cp -r \"outputs/{name}\" \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "\n",
    "# Copy combined results\n",
    "!cp outputs/colab_all_results.json \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "!cp outputs/colab_training_comparison.png \"{DRIVE_DATA_PATH}/outputs/\"\n",
    "\n",
    "print(\"\\n‚úì All results saved to Google Drive!\")\n",
    "print(f\"Location: {DRIVE_DATA_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
